{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, add, concatenate\n",
    "from keras.layers import CuDNNLSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import LearningRateScheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EMBEDDING_FILES = [\n",
    "    '../input/fasttext-crawl-300d-2m/crawl-300d-2M.vec',\n",
    "    '../input/glove840b300dtxt/glove.840B.300d.txt'\n",
    "]\n",
    "NUM_MODELS = 2\n",
    "BATCH_SIZE = 512\n",
    "LSTM_UNITS = 128\n",
    "DENSE_HIDDEN_UNITS = 4 * LSTM_UNITS\n",
    "EPOCHS = 4\n",
    "MAX_LEN = 220\n",
    "IDENTITY_COLUMNS = [\n",
    "    'male', 'female', 'homosexual_gay_or_lesbian', 'christian', 'jewish',\n",
    "    'muslim', 'black', 'white', 'psychiatric_or_mental_illness'\n",
    "]\n",
    "AUX_COLUMNS = ['target', 'severe_toxicity', 'obscene', 'identity_attack', 'insult', 'threat']\n",
    "TEXT_COLUMN = 'comment_text'\n",
    "TARGET_COLUMN = 'target'\n",
    "CHARS_TO_REMOVE = '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n“”’\\'∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_coefs(word, *arr):\n",
    "    return word, np.asarray(arr, dtype='float32')\n",
    "\n",
    "\n",
    "def load_embeddings(path):\n",
    "    with open(path) as f:\n",
    "        return dict(get_coefs(*line.strip().split(' ')) for line in f)\n",
    "\n",
    "\n",
    "def build_matrix(word_index, path):\n",
    "    embedding_index = load_embeddings(path)\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, 300))\n",
    "    for word, i in word_index.items():\n",
    "        try:\n",
    "            embedding_matrix[i] = embedding_index[word]\n",
    "        except KeyError:\n",
    "            pass\n",
    "    return embedding_matrix\n",
    "    \n",
    "\n",
    "def build_model(embedding_matrix, num_aux_targets):\n",
    "    words = Input(shape=(None,))\n",
    "    x = Embedding(*embedding_matrix.shape, weights=[embedding_matrix], trainable=False)(words)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "    x = Bidirectional(CuDNNLSTM(LSTM_UNITS, return_sequences=True))(x)\n",
    "\n",
    "    hidden = concatenate([\n",
    "        GlobalMaxPooling1D()(x),\n",
    "        GlobalAveragePooling1D()(x),\n",
    "    ])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    hidden = add([hidden, Dense(DENSE_HIDDEN_UNITS, activation='relu')(hidden)])\n",
    "    result = Dense(1, activation='sigmoid')(hidden)\n",
    "    aux_result = Dense(num_aux_targets, activation='sigmoid')(hidden)\n",
    "    \n",
    "    model = Model(inputs=words, outputs=[result, aux_result])\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "\n",
    "    return model\n",
    "    \n",
    "\n",
    "train_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv')\n",
    "test_df = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv')\n",
    "\n",
    "x_train = train_df[TEXT_COLUMN].astype(str)\n",
    "y_train = train_df[TARGET_COLUMN].values\n",
    "y_aux_train = train_df[AUX_COLUMNS].values\n",
    "x_test = test_df[TEXT_COLUMN].astype(str)\n",
    "\n",
    "for column in IDENTITY_COLUMNS + [TARGET_COLUMN]:\n",
    "    train_df[column] = np.where(train_df[column] >= 0.5, True, False)\n",
    "\n",
    "tokenizer = text.Tokenizer(filters=CHARS_TO_REMOVE)\n",
    "tokenizer.fit_on_texts(list(x_train) + list(x_test))\n",
    "\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "x_train = sequence.pad_sequences(x_train, maxlen=MAX_LEN)\n",
    "x_test = sequence.pad_sequences(x_test, maxlen=MAX_LEN)\n",
    "\n",
    "sample_weights = np.ones(len(x_train), dtype=np.float32)\n",
    "sample_weights += train_df[IDENTITY_COLUMNS].sum(axis=1)\n",
    "sample_weights += train_df[TARGET_COLUMN] * (~train_df[IDENTITY_COLUMNS]).sum(axis=1)\n",
    "sample_weights += (~train_df[TARGET_COLUMN]) * train_df[IDENTITY_COLUMNS].sum(axis=1) * 5\n",
    "sample_weights /= sample_weights.mean()\n",
    "\n",
    "embedding_matrix = np.concatenate(\n",
    "    [build_matrix(tokenizer.word_index, f) for f in EMBEDDING_FILES], axis=-1)\n",
    "\n",
    "checkpoint_predictions = []\n",
    "weights = []\n",
    "\n",
    "for model_idx in range(NUM_MODELS):\n",
    "    model = build_model(embedding_matrix, y_aux_train.shape[-1])\n",
    "    for global_epoch in range(EPOCHS):\n",
    "        model.fit(\n",
    "            x_train,\n",
    "            [y_train, y_aux_train],\n",
    "            batch_size=BATCH_SIZE,\n",
    "            epochs=1,\n",
    "            verbose=2,\n",
    "            sample_weight=[sample_weights.values, np.ones_like(sample_weights)],\n",
    "            callbacks=[\n",
    "                LearningRateScheduler(lambda _: 1e-3 * (0.55 ** global_epoch))\n",
    "            ]\n",
    "        )\n",
    "        checkpoint_predictions.append(model.predict(x_test, batch_size=2048)[0].flatten())\n",
    "        weights.append(2 ** global_epoch)\n",
    "\n",
    "predictions = np.average(checkpoint_predictions, weights=weights, axis=0)\n",
    "\n",
    "submission = pd.DataFrame.from_dict({\n",
    "    'id': test_df.id,\n",
    "    'prediction': predictions\n",
    "})\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = pd.read_csv('D:/kaggle/toxic/features/X_train.csv')\n",
    "X_test = pd.read_csv('D:/kaggle/toxic/features/X_test.csv')\n",
    "y = pd.read_csv('D:/kaggle/toxic/features/y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>LSTM_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.021771</td>\n",
       "      <td>0.001325</td>\n",
       "      <td>0.002755</td>\n",
       "      <td>0.001891</td>\n",
       "      <td>0.014582</td>\n",
       "      <td>0.002988</td>\n",
       "      <td>0.036285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.021884</td>\n",
       "      <td>0.001487</td>\n",
       "      <td>0.002501</td>\n",
       "      <td>0.002838</td>\n",
       "      <td>0.012895</td>\n",
       "      <td>0.004374</td>\n",
       "      <td>0.030298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.007624</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.001333</td>\n",
       "      <td>0.001032</td>\n",
       "      <td>0.005177</td>\n",
       "      <td>0.001236</td>\n",
       "      <td>0.008271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.014251</td>\n",
       "      <td>0.001551</td>\n",
       "      <td>0.001337</td>\n",
       "      <td>0.001581</td>\n",
       "      <td>0.003890</td>\n",
       "      <td>0.009391</td>\n",
       "      <td>0.023089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.799620</td>\n",
       "      <td>0.025370</td>\n",
       "      <td>0.037805</td>\n",
       "      <td>0.064720</td>\n",
       "      <td>0.795880</td>\n",
       "      <td>0.003541</td>\n",
       "      <td>0.804656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target  severe_toxicity   obscene  identity_attack    insult    threat  \\\n",
       "0  0.021771         0.001325  0.002755         0.001891  0.014582  0.002988   \n",
       "1  0.021884         0.001487  0.002501         0.002838  0.012895  0.004374   \n",
       "2  0.007624         0.000605  0.001333         0.001032  0.005177  0.001236   \n",
       "3  0.014251         0.001551  0.001337         0.001581  0.003890  0.009391   \n",
       "4  0.799620         0.025370  0.037805         0.064720  0.795880  0.003541   \n",
       "\n",
       "   LSTM_pred  \n",
       "0   0.036285  \n",
       "1   0.030298  \n",
       "2   0.008271  \n",
       "3   0.023089  \n",
       "4   0.804656  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>severe_toxicity</th>\n",
       "      <th>obscene</th>\n",
       "      <th>identity_attack</th>\n",
       "      <th>insult</th>\n",
       "      <th>threat</th>\n",
       "      <th>LSTM_pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.030749</td>\n",
       "      <td>0.001290</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.002900</td>\n",
       "      <td>0.025883</td>\n",
       "      <td>0.001784</td>\n",
       "      <td>0.040212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.007482</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.001511</td>\n",
       "      <td>0.002551</td>\n",
       "      <td>0.004234</td>\n",
       "      <td>0.001208</td>\n",
       "      <td>0.006855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.093828</td>\n",
       "      <td>0.003138</td>\n",
       "      <td>0.003488</td>\n",
       "      <td>0.026470</td>\n",
       "      <td>0.069795</td>\n",
       "      <td>0.009624</td>\n",
       "      <td>0.156916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.043771</td>\n",
       "      <td>0.001783</td>\n",
       "      <td>0.003066</td>\n",
       "      <td>0.003591</td>\n",
       "      <td>0.035355</td>\n",
       "      <td>0.003025</td>\n",
       "      <td>0.052264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.763139</td>\n",
       "      <td>0.025338</td>\n",
       "      <td>0.068321</td>\n",
       "      <td>0.020725</td>\n",
       "      <td>0.756949</td>\n",
       "      <td>0.008225</td>\n",
       "      <td>0.751200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     target  severe_toxicity   obscene  identity_attack    insult    threat  \\\n",
       "0  0.030749         0.001290  0.002378         0.002900  0.025883  0.001784   \n",
       "1  0.007482         0.000714  0.001511         0.002551  0.004234  0.001208   \n",
       "2  0.093828         0.003138  0.003488         0.026470  0.069795  0.009624   \n",
       "3  0.043771         0.001783  0.003066         0.003591  0.035355  0.003025   \n",
       "4  0.763139         0.025338  0.068321         0.020725  0.756949  0.008225   \n",
       "\n",
       "   LSTM_pred  \n",
       "0   0.040212  \n",
       "1   0.006855  \n",
       "2   0.156916  \n",
       "3   0.052264  \n",
       "4   0.751200  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost as xgb\n",
    "import lightgbm as  lgb\n",
    "from sklearn.model_selection import KFold\n",
    "from hyperopt import STATUS_OK, tpe, hp, Trials, fmin\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "NUM_EVALS = 1000\n",
    "N_FOLDS = 5\n",
    "XGB_MAX_LEAVES = 2**12 \n",
    "XGB_MAX_DEPTH = 50\n",
    "EVAL_METRIC_XGB_REG = 'mae'\n",
    "LGBM_MAX_LEAVES = 2**11\n",
    "LGBM_MAX_DEPTH = 35\n",
    "EVAL_METRIC_LGBM_REG = 'mae'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_hyperopt(data, labels, package='lgbm', num_evals=NUM_EVALS, diagnostic=False):\n",
    "    \n",
    "    #==========\n",
    "    #LightGBM\n",
    "    #==========\n",
    "    \n",
    "    if package=='lgbm':\n",
    "        \n",
    "        print('Running {} rounds of LightGBM parameter optimisation:'.format(num_evals))\n",
    "        #clear space\n",
    "        gc.collect()\n",
    "        \n",
    "        integer_params = ['max_depth',\n",
    "                          'num_leaves',\n",
    "                          'max_bin',\n",
    "                          'min_data_in_leaf',\n",
    "                          'min_data_in_bin']\n",
    "        \n",
    "        def objective(space_params):\n",
    "            \n",
    "            #cast integer params from float to int\n",
    "            for param in integer_params:\n",
    "                space_params[param] = int(space_params[param])\n",
    "            \n",
    "            #extract nested conditional parameters\n",
    "            if space_params['boosting']['boosting'] == 'goss':\n",
    "                top_rate = space_params['boosting'].get('top_rate')\n",
    "                other_rate = space_params['boosting'].get('other_rate')\n",
    "                #0 <= top_rate + other_rate <= 1\n",
    "                top_rate = max(top_rate, 0)\n",
    "                top_rate = min(top_rate, 0.5)\n",
    "                other_rate = max(other_rate, 0)\n",
    "                other_rate = min(other_rate, 0.5)\n",
    "                space_params['top_rate'] = top_rate\n",
    "                space_params['other_rate'] = other_rate\n",
    "            \n",
    "            subsample = space_params['boosting'].get('subsample', 1.0)\n",
    "            space_params['boosting'] = space_params['boosting']['boosting']\n",
    "            space_params['subsample'] = subsample\n",
    "            \n",
    "            #for classification, set stratified=True and metrics=EVAL_METRIC_LGBM_CLASS\n",
    "            cv_results = lgb.cv(space_params, train, num_boost_round=100, nfold = N_FOLDS, stratified=False,\n",
    "                                early_stopping_rounds=100, metrics=EVAL_METRIC_LGBM_REG, seed=42)\n",
    "            \n",
    "            best_loss = cv_results['l1-mean'][-1] #'l2-mean' for rmse\n",
    "            #for classification, comment out the line above and uncomment the line below:\n",
    "            #best_loss = 1 - cv_results['auc-mean'][-1]\n",
    "            #if necessary, replace 'auc-mean' with '[your-preferred-metric]-mean'\n",
    "            return{'loss':best_loss, 'status': STATUS_OK }\n",
    "        \n",
    "        train = lgb.Dataset(data, labels)\n",
    "                \n",
    "        #integer and string parameters, used with hp.choice()\n",
    "        boosting_list = [{'boosting': 'gbdt',\n",
    "                          'subsample': hp.uniform('subsample', 0.5, 1)},\n",
    "                         {'boosting': 'goss',\n",
    "                          'subsample': 1.0,\n",
    "                         'top_rate': hp.uniform('top_rate', 0, 0.5),\n",
    "                         'other_rate': hp.uniform('other_rate', 0, 0.5)}] #if including 'dart', make sure to set 'n_estimators'\n",
    "        metric_list = ['MAE'] \n",
    "        #for classification comment out the line above and uncomment the line below\n",
    "        #metric_list = ['auc'] #modify as required for other classification metrics\n",
    "        objective_list_reg = ['huber',  'gamma', 'tweedie', 'fair']\n",
    "        objective_list_class = ['binary', 'cross_entropy']\n",
    "        #for classification set objective_list = objective_list_class\n",
    "        objective_list = objective_list_reg\n",
    "\n",
    "        space ={'boosting' : hp.choice('boosting', boosting_list),\n",
    "                'num_leaves' : hp.quniform('num_leaves', 2, LGBM_MAX_LEAVES, 1),\n",
    "                'max_depth': hp.quniform('max_depth', 2, LGBM_MAX_DEPTH, 1),\n",
    "                'max_bin': hp.quniform('max_bin', 32, 255, 1),\n",
    "                'min_data_in_leaf': hp.quniform('min_data_in_leaf', 1, 256, 1),\n",
    "                'min_data_in_bin': hp.quniform('min_data_in_bin', 1, 256, 1),\n",
    "                'min_gain_to_split' : hp.quniform('min_gain_to_split', 0.1, 5, 0.01),\n",
    "                'lambda_l1' : hp.uniform('lambda_l1', 0, 5),\n",
    "                'lambda_l2' : hp.uniform('lambda_l2', 0, 5),\n",
    "                'learning_rate' : hp.loguniform('learning_rate', np.log(0.005), np.log(0.2)),\n",
    "                'metric' : hp.choice('metric', metric_list),\n",
    "                'objective' : hp.choice('objective', objective_list),\n",
    "                'feature_fraction' : hp.quniform('feature_fraction', 0.5, 1, 0.01),\n",
    "                'bagging_fraction' : hp.quniform('bagging_fraction', 0.5, 1, 0.01)\n",
    "            }\n",
    "        \n",
    "        #optional: activate GPU for LightGBM\n",
    "        #follow compilation steps here:\n",
    "        #https://www.kaggle.com/vinhnguyen/gpu-acceleration-for-lightgbm/\n",
    "        #then uncomment lines below:\n",
    "        #space['device'] = 'gpu'\n",
    "        #space['gpu_platform_id'] = 0,\n",
    "        #space['gpu_device_id'] =  0\n",
    "\n",
    "        trials = Trials()\n",
    "        best = fmin(fn=objective,\n",
    "                    space=space,\n",
    "                    algo=tpe.suggest,\n",
    "                    max_evals=num_evals, \n",
    "                    trials=trials)\n",
    "                \n",
    "        #fmin() will return the index of values chosen from the lists/arrays in 'space'\n",
    "        #to obtain actual values, index values are used to subset the original lists/arrays\n",
    "        best['boosting'] = boosting_list[best['boosting']]['boosting']#nested dict, index twice\n",
    "        best['metric'] = metric_list[best['metric']]\n",
    "        best['objective'] = objective_list[best['objective']]\n",
    "                \n",
    "        #cast floats of integer params to int\n",
    "        for param in integer_params:\n",
    "            best[param] = int(best[param])\n",
    "        \n",
    "        print('{' + '\\n'.join('{}: {}'.format(k, v) for k, v in best.items()) + '}')\n",
    "        if diagnostic:\n",
    "            return(best, trials)\n",
    "        else:\n",
    "            return(best)\n",
    "    \n",
    "    #==========\n",
    "    #XGBoost\n",
    "    #==========\n",
    "    \n",
    "    if package=='xgb':\n",
    "        \n",
    "        print('Running {} rounds of XGBoost parameter optimisation:'.format(num_evals))\n",
    "        #clear space\n",
    "        gc.collect()\n",
    "        \n",
    "        integer_params = ['max_depth']\n",
    "        \n",
    "        def objective(space_params):\n",
    "            \n",
    "            for param in integer_params:\n",
    "                space_params[param] = int(space_params[param])\n",
    "                \n",
    "            #extract multiple nested tree_method conditional parameters\n",
    "            #libera te tutemet ex inferis\n",
    "            if space_params['tree_method']['tree_method'] == 'hist':\n",
    "                max_bin = space_params['tree_method'].get('max_bin')\n",
    "                space_params['max_bin'] = int(max_bin)\n",
    "                if space_params['tree_method']['grow_policy']['grow_policy']['grow_policy'] == 'depthwise':\n",
    "                    grow_policy = space_params['tree_method'].get('grow_policy').get('grow_policy').get('grow_policy')\n",
    "                    space_params['grow_policy'] = grow_policy\n",
    "                    space_params['tree_method'] = 'hist'\n",
    "                else:\n",
    "                    max_leaves = space_params['tree_method']['grow_policy']['grow_policy'].get('max_leaves')\n",
    "                    space_params['grow_policy'] = 'lossguide'\n",
    "                    space_params['max_leaves'] = int(max_leaves)\n",
    "                    space_params['tree_method'] = 'hist'\n",
    "            else:\n",
    "                space_params['tree_method'] = space_params['tree_method'].get('tree_method')\n",
    "                \n",
    "            #for classification replace EVAL_METRIC_XGB_REG with EVAL_METRIC_XGB_CLASS\n",
    "            cv_results = xgb.cv(space_params, train, num_boost_round=100, nfold=N_FOLDS, metrics=[EVAL_METRIC_XGB_REG],\n",
    "                             early_stopping_rounds=100, stratified=False, seed=42)\n",
    "            \n",
    "            best_loss = cv_results['test-mae-mean'].iloc[-1] #or 'test-rmse-mean' if using RMSE\n",
    "            #for classification, comment out the line above and uncomment the line below:\n",
    "            #best_loss = 1 - cv_results['test-auc-mean'].iloc[-1]\n",
    "            #if necessary, replace 'test-auc-mean' with 'test-[your-preferred-metric]-mean'\n",
    "            return{'loss':best_loss, 'status': STATUS_OK }\n",
    "        \n",
    "        train = xgb.DMatrix(data, labels)\n",
    "        \n",
    "        #integer and string parameters, used with hp.choice()\n",
    "        boosting_list = ['gbtree', 'gblinear'] #if including 'dart', make sure to set 'n_estimators'\n",
    "        metric_list = ['mae'] \n",
    "        #for classification comment out the line above and uncomment the line below\n",
    "        #metric_list = ['auc']\n",
    "        #modify as required for other classification metrics classification\n",
    "        \n",
    "        tree_method = [{'tree_method' : 'exact'},\n",
    "               {'tree_method' : 'approx'},\n",
    "               {'tree_method' : 'hist',\n",
    "                'max_bin': hp.quniform('max_bin', 2**3, 2**7, 1),\n",
    "                'grow_policy' : {'grow_policy': {'grow_policy':'depthwise'},\n",
    "                                'grow_policy' : {'grow_policy':'lossguide',\n",
    "                                                  'max_leaves': hp.quniform('max_leaves', 32, XGB_MAX_LEAVES, 1)}}}]\n",
    "        \n",
    "        #if using GPU, replace 'exact' with 'gpu_exact' and 'hist' with\n",
    "        #'gpu_hist' in the nested dictionary above\n",
    "        \n",
    "        objective_list_reg = ['reg:linear']\n",
    "        objective_list_class = ['reg:logistic', 'binary:logistic']\n",
    "        #for classification change line below to 'objective_list = objective_list_class'\n",
    "        objective_list = objective_list_reg\n",
    "        \n",
    "        space ={'boosting' : hp.choice('boosting', boosting_list),\n",
    "                'tree_method' : hp.choice('tree_method', tree_method),\n",
    "                'max_depth': hp.quniform('max_depth', 2, XGB_MAX_DEPTH, 1),\n",
    "                'reg_alpha' : hp.uniform('reg_alpha', 0, 5),\n",
    "                'reg_lambda' : hp.uniform('reg_lambda', 0, 5),\n",
    "                'min_child_weight' : hp.uniform('min_child_weight', 0, 5),\n",
    "                'gamma' : hp.uniform('gamma', 0, 5),\n",
    "                'learning_rate' : hp.loguniform('learning_rate', np.log(0.005), np.log(0.2)),\n",
    "                'eval_metric' : hp.choice('eval_metric', metric_list),\n",
    "                'objective' : hp.choice('objective', objective_list),\n",
    "                'colsample_bytree' : hp.quniform('colsample_bytree', 0.1, 1, 0.01),\n",
    "                'colsample_bynode' : hp.quniform('colsample_bynode', 0.1, 1, 0.01),\n",
    "                'colsample_bylevel' : hp.quniform('colsample_bylevel', 0.1, 1, 0.01),\n",
    "                'subsample' : hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "                'nthread' : 3\n",
    "            }\n",
    "        \n",
    "        trials = Trials()\n",
    "        best = fmin(fn=objective,\n",
    "                    space=space,\n",
    "                    algo=tpe.suggest,\n",
    "                    max_evals=num_evals, \n",
    "                    trials=trials)\n",
    "        \n",
    "        best['tree_method'] = tree_method[best['tree_method']]['tree_method']\n",
    "        best['boosting'] = boosting_list[best['boosting']]\n",
    "        best['eval_metric'] = metric_list[best['eval_metric']]\n",
    "        best['objective'] = objective_list[best['objective']]\n",
    "        \n",
    "        #cast floats of integer params to int\n",
    "        for param in integer_params:\n",
    "            best[param] = int(best[param])\n",
    "        if 'max_leaves' in best:\n",
    "            best['max_leaves'] = int(best['max_leaves'])\n",
    "        if 'max_bin' in best:\n",
    "            best['max_bin'] = int(best['max_bin'])\n",
    "        \n",
    "        print('{' + '\\n'.join('{}: {}'.format(k, v) for k, v in best.items()) + '}')\n",
    "        \n",
    "        if diagnostic:\n",
    "            return(best, trials)\n",
    "        else:\n",
    "            return(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 1000 rounds of LightGBM parameter optimisation:\n",
      "100%|█| 1000/1000 [13:57:39<00:00, 46.36s/it, best loss: 0.06612121983943806] \n",
      "{bagging_fraction: 0.99\n",
      "boosting: goss\n",
      "feature_fraction: 0.88\n",
      "lambda_l1: 1.1650370101495848\n",
      "lambda_l2: 2.2303825287613672\n",
      "learning_rate: 0.045844295771991894\n",
      "max_bin: 249\n",
      "max_depth: 35\n",
      "metric: MAE\n",
      "min_data_in_bin: 73\n",
      "min_data_in_leaf: 1\n",
      "min_gain_to_split: 3.5100000000000002\n",
      "num_leaves: 1980\n",
      "objective: gamma\n",
      "other_rate: 0.1642876753001118\n",
      "top_rate: 9.784632667626109e-05}\n"
     ]
    }
   ],
   "source": [
    "lgb_params = quick_hyperopt(X_train, y, 'lgbm', 1000, diagnostic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "file = open('C:/Users/Administrator/Desktop/kaggle/word_dict.txt', 'r') \n",
    "js = file.read()\n",
    "dic = json.loads(js)   \n",
    "file.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'This': 1,\n",
       " 'is': 2,\n",
       " 'so': 3,\n",
       " 'cool': 4,\n",
       " '.': 5,\n",
       " 'It': 6,\n",
       " \"'s\": 7,\n",
       " 'like': 8,\n",
       " ',': 9,\n",
       " \"'\": 10,\n",
       " 'would': 11,\n",
       " 'you': 12,\n",
       " 'want': 13,\n",
       " 'your': 14,\n",
       " 'mother': 15,\n",
       " 'to': 16,\n",
       " 'read': 17,\n",
       " 'this': 18,\n",
       " '?': 19,\n",
       " 'Really': 20,\n",
       " 'great': 21,\n",
       " 'idea': 22,\n",
       " 'well': 23,\n",
       " 'done': 24,\n",
       " '!': 25,\n",
       " 'Thank': 26,\n",
       " 'make': 27,\n",
       " 'my': 28,\n",
       " 'life': 29,\n",
       " 'a': 30,\n",
       " 'lot': 31,\n",
       " 'less': 32,\n",
       " 'anxiety': 33,\n",
       " '-': 34,\n",
       " 'inducing': 35,\n",
       " 'Keep': 36,\n",
       " 'it': 37,\n",
       " 'up': 38,\n",
       " 'and': 39,\n",
       " 'do': 40,\n",
       " 'not': 41,\n",
       " 'let': 42,\n",
       " 'anyone': 43,\n",
       " 'get': 44,\n",
       " 'in': 45,\n",
       " 'way': 46,\n",
       " 'such': 47,\n",
       " 'an': 48,\n",
       " 'urgent': 49,\n",
       " 'design': 50,\n",
       " 'problem': 51,\n",
       " ';': 52,\n",
       " 'kudos': 53,\n",
       " 'for': 54,\n",
       " 'taking': 55,\n",
       " 'on': 56,\n",
       " 'Very': 57,\n",
       " 'impressive': 58,\n",
       " 'Is': 59,\n",
       " 'something': 60,\n",
       " 'I': 61,\n",
       " 'will': 62,\n",
       " 'be': 63,\n",
       " 'able': 64,\n",
       " 'install': 65,\n",
       " 'site': 66,\n",
       " 'When': 67,\n",
       " 'releasing': 68,\n",
       " 'haha': 69,\n",
       " 'guys': 70,\n",
       " 'are': 71,\n",
       " 'bunch': 72,\n",
       " 'of': 73,\n",
       " 'losers': 74,\n",
       " 'ur': 75,\n",
       " 'sh*tty': 76,\n",
       " 'comment': 77,\n",
       " 'hahahahahahahahhha': 78,\n",
       " 'suck': 79,\n",
       " 'FFFFUUUUUUUUUUUUUUU': 80,\n",
       " 'The': 81,\n",
       " 'ranchers': 82,\n",
       " 'seem': 83,\n",
       " 'motivated': 84,\n",
       " 'by': 85,\n",
       " 'mostly': 86,\n",
       " 'greed': 87,\n",
       " 'no': 88,\n",
       " 'one': 89,\n",
       " 'should': 90,\n",
       " 'have': 91,\n",
       " 'the': 92,\n",
       " 'right': 93,\n",
       " 'allow': 94,\n",
       " 'their': 95,\n",
       " 'animals': 96,\n",
       " 'destroy': 97,\n",
       " 'public': 98,\n",
       " 'land': 99,\n",
       " 'was': 100,\n",
       " 'show': 101,\n",
       " 'Not': 102,\n",
       " 'combo': 103,\n",
       " 'expected': 104,\n",
       " 'good': 105,\n",
       " 'together': 106,\n",
       " 'but': 107,\n",
       " 'Wow': 108,\n",
       " 'that': 109,\n",
       " 'sounds': 110,\n",
       " 'story': 111,\n",
       " 'Man': 112,\n",
       " 'wonder': 113,\n",
       " 'if': 114,\n",
       " 'person': 115,\n",
       " 'who': 116,\n",
       " 'yelled': 117,\n",
       " '\"': 118,\n",
       " 'shut': 119,\n",
       " 'fuck': 120,\n",
       " 'at': 121,\n",
       " 'him': 122,\n",
       " 'ever': 123,\n",
       " 'heard': 124,\n",
       " 'seems': 125,\n",
       " 'step': 126,\n",
       " 'direction': 127,\n",
       " 'ridiculous': 128,\n",
       " 'these': 129,\n",
       " 'being': 130,\n",
       " 'called': 131,\n",
       " 'protesters': 132,\n",
       " 'Being': 133,\n",
       " 'armed': 134,\n",
       " 'threat': 135,\n",
       " 'violence': 136,\n",
       " 'which': 137,\n",
       " 'makes': 138,\n",
       " 'them': 139,\n",
       " 'terrorists': 140,\n",
       " 'gets': 141,\n",
       " 'more': 142,\n",
       " 'hour': 143,\n",
       " 'And': 144,\n",
       " 'love': 145,\n",
       " 'people': 146,\n",
       " 'sending': 147,\n",
       " 'dildos': 148,\n",
       " 'mail': 149,\n",
       " 'now': 150,\n",
       " 'But': 151,\n",
       " '…': 152,\n",
       " 'they': 153,\n",
       " 'really': 154,\n",
       " 'think': 155,\n",
       " 'there': 156,\n",
       " 'happy': 157,\n",
       " 'ending': 158,\n",
       " 'any': 159,\n",
       " 'even': 160,\n",
       " 'deluded': 161,\n",
       " 'than': 162,\n",
       " 'all': 163,\n",
       " 'jokes': 164,\n",
       " 'about': 165,\n",
       " 'assume': 166,\n",
       " 'agree': 167,\n",
       " 'grant': 168,\n",
       " 'legitimacy': 169,\n",
       " 'protestors': 170,\n",
       " 'They': 171,\n",
       " \"'re\": 172,\n",
       " 'greedy': 173,\n",
       " 'small': 174,\n",
       " 'minded': 175,\n",
       " 'somehow': 176,\n",
       " 'share': 177,\n",
       " 'mass': 178,\n",
       " 'delusion': 179,\n",
       " 'only': 180,\n",
       " 'themselves': 181,\n",
       " 'as': 182,\n",
       " 'individuals': 183,\n",
       " 'thing': 184,\n",
       " 'large': 185,\n",
       " 'Basically': 186,\n",
       " ':': 187,\n",
       " 'take': 188,\n",
       " 'currently': 189,\n",
       " 'belongs': 190,\n",
       " 'everyone': 191,\n",
       " 'give': 192,\n",
       " 'select': 193,\n",
       " 'group': 194,\n",
       " 'can': 195,\n",
       " 'profit': 196,\n",
       " 'Interesting': 197,\n",
       " 'curious': 198,\n",
       " 'see': 199,\n",
       " 'how': 200,\n",
       " 'works': 201,\n",
       " 'out': 202,\n",
       " 'often': 203,\n",
       " 'refrain': 204,\n",
       " 'from': 205,\n",
       " 'commenting': 206,\n",
       " 'because': 207,\n",
       " 'time': 208,\n",
       " 'or': 209,\n",
       " 'desire': 210,\n",
       " 'engage': 211,\n",
       " 'with': 212,\n",
       " 'couple': 213,\n",
       " 'resident': 214,\n",
       " 'trolls': 215,\n",
       " 'jump': 216,\n",
       " 'every': 217,\n",
       " 'active': 218,\n",
       " 'WW': 219,\n",
       " 'thread': 220,\n",
       " 'Awesome': 221,\n",
       " 'Civil': 222,\n",
       " 'Comments': 223,\n",
       " 'am': 224,\n",
       " 'glad': 225,\n",
       " 'working': 226,\n",
       " 'look': 227,\n",
       " 'forward': 228,\n",
       " 'seeing': 229,\n",
       " 'plays': 230,\n",
       " '   ': 231,\n",
       " 'comments': 232,\n",
       " 'sections': 233,\n",
       " 'online': 234,\n",
       " 'news': 235,\n",
       " 'stories': 236,\n",
       " 'potential': 237,\n",
       " 'tools': 238,\n",
       " 'community': 239,\n",
       " 'interaction': 240,\n",
       " 'current': 241,\n",
       " 'events': 242,\n",
       " 'Neo': 243,\n",
       " 'Town': 244,\n",
       " 'Hall': 245,\n",
       " 'sorts': 246,\n",
       " ' ': 247,\n",
       " 'One': 248,\n",
       " 'reasons': 249,\n",
       " 'rely': 250,\n",
       " 'Reddit': 251,\n",
       " 'platform': 252,\n",
       " 'local': 253,\n",
       " 'discussions': 254,\n",
       " 'sense': 255,\n",
       " 'lacking': 256,\n",
       " 'real': 257,\n",
       " 'hectic': 258,\n",
       " ' \\n\\n': 259,\n",
       " 'hopefully': 260,\n",
       " 'we': 261,\n",
       " 'tempted': 262,\n",
       " 'silence': 263,\n",
       " 'those': 264,\n",
       " 'unpopular': 265,\n",
       " 'stances': 266,\n",
       " 'Angry': 267,\n",
       " 'misogynists': 268,\n",
       " 'Racists': 269,\n",
       " 'oh': 270,\n",
       " 'does': 271,\n",
       " '150': 272,\n",
       " 'IQ': 273,\n",
       " 'slant': 274,\n",
       " 'here': 275,\n",
       " 'Diversity': 276,\n",
       " 'diode': 277,\n",
       " 'work': 278,\n",
       " 'yet': 279,\n",
       " 'again': 280,\n",
       " 'We': 281,\n",
       " 'say': 282,\n",
       " 'anything': 283,\n",
       " 'You': 284,\n",
       " 'other': 285,\n",
       " 'hand': 286,\n",
       " 'must': 287,\n",
       " 'what': 288,\n",
       " 'From': 289,\n",
       " 'winning': 290,\n",
       " 'arguments': 291,\n",
       " 'against': 292,\n",
       " 'member': 293,\n",
       " 'diversity': 294,\n",
       " 'considered': 295,\n",
       " 'offensive': 296,\n",
       " 'language': 297,\n",
       " 'facts': 298,\n",
       " 'cogent': 299,\n",
       " 'linear': 300,\n",
       " 'posts': 301,\n",
       " 'Math': 302,\n",
       " 'verboten': 303,\n",
       " 'Nice': 304,\n",
       " 'some': 305,\n",
       " 'attempts': 306,\n",
       " 'try': 307,\n",
       " 'better': 308,\n",
       " '—': 309,\n",
       " 'feels': 310,\n",
       " 'innovation': 311,\n",
       " 'communities': 312,\n",
       " 'ended': 313,\n",
       " 'launch': 314,\n",
       " 'Disqus': 315,\n",
       " 'nearly': 316,\n",
       " 'decade': 317,\n",
       " 'ago': 318,\n",
       " 'hope': 319,\n",
       " 'purpose': 320,\n",
       " 'introducing': 321,\n",
       " 'system': 322,\n",
       " 'encourage': 323,\n",
       " 'debate': 324,\n",
       " 'discussion': 325,\n",
       " 'several': 326,\n",
       " 'things': 327,\n",
       " 'limit': 328,\n",
       " 'flow': 329,\n",
       " 'Making': 330,\n",
       " 'personal': 331,\n",
       " 'attacks': 332,\n",
       " 'encouraging': 333,\n",
       " 'witch': 334,\n",
       " 'hunts': 335,\n",
       " 'true': 336,\n",
       " 'spam': 337,\n",
       " '(': 338,\n",
       " 'has': 339,\n",
       " 'been': 340,\n",
       " 'section': 341,\n",
       " 'experience': 342,\n",
       " ')': 343,\n",
       " 'chasing': 344,\n",
       " 'rabbits': 345,\n",
       " 'draw': 346,\n",
       " 'away': 347,\n",
       " 'primary': 348,\n",
       " 'subject': 349,\n",
       " '\\n\\n': 350,\n",
       " 'opinions': 351,\n",
       " 'silenced': 352,\n",
       " 'Healthy': 353,\n",
       " 'important': 354,\n",
       " 'component': 355,\n",
       " 'civil': 356,\n",
       " 'society': 357,\n",
       " 'when': 358,\n",
       " 'degenerate': 359,\n",
       " 'into': 360,\n",
       " 'insults': 361,\n",
       " 'distraction': 362,\n",
       " 'actually': 363,\n",
       " 'limits': 364,\n",
       " 'ability': 365,\n",
       " 'talk': 366,\n",
       " 'each': 367,\n",
       " 'believing': 368,\n",
       " 'intention': 369,\n",
       " 'newspaper': 370,\n",
       " 'movement': 371,\n",
       " 'results': 372,\n",
       " 'commentary': 373,\n",
       " 'need': 374,\n",
       " 'randomly': 375,\n",
       " 'chosen': 376,\n",
       " 'reviewed': 377,\n",
       " 'bet': 378,\n",
       " 'thorough': 379,\n",
       " 'answer': 380,\n",
       " 'question': 381,\n",
       " 'She': 382,\n",
       " 'major': 383,\n",
       " 'improvement': 384,\n",
       " 'city': 385,\n",
       " 'council': 386,\n",
       " 'she': 387,\n",
       " 'long': 388,\n",
       " 'history': 389,\n",
       " 'giving': 390,\n",
       " 'citizens': 391,\n",
       " 'voice': 392,\n",
       " 'solver': 393,\n",
       " 'Portland': 394,\n",
       " 'needs': 395,\n",
       " 'much': 396,\n",
       " 'places': 397,\n",
       " 'especially': 398,\n",
       " 'publications': 399,\n",
       " 'already': 400,\n",
       " 'serve': 401,\n",
       " 'amazing': 402,\n",
       " 'then': 403,\n",
       " 'biased': 404,\n",
       " \"I'm\": 405,\n",
       " 'co': 406,\n",
       " 'founder': 407,\n",
       " \"'ve\": 408,\n",
       " 'worked': 409,\n",
       " 'hard': 410,\n",
       " 'sure': 411,\n",
       " 'become': 412,\n",
       " 'echo': 413,\n",
       " 'chamber': 414,\n",
       " 'though': 415,\n",
       " 'combining': 416,\n",
       " 'clever': 417,\n",
       " 'algorithms': 418,\n",
       " 'backend': 419,\n",
       " 'peer': 420,\n",
       " 'reviews': 421,\n",
       " 'That': 422,\n",
       " 'also': 423,\n",
       " 'why': 424,\n",
       " 'two': 425,\n",
       " 'separate': 426,\n",
       " 'questions': 427,\n",
       " '*': 428,\n",
       " 'Great': 429,\n",
       " 'asked': 430,\n",
       " 'designed': 431,\n",
       " 'assuming': 432,\n",
       " 'abuse': 433,\n",
       " 'So': 434,\n",
       " 'addition': 435,\n",
       " 'doing': 436,\n",
       " 'meta': 437,\n",
       " 'analysis': 438,\n",
       " '100': 439,\n",
       " '%': 440,\n",
       " 'perfect': 441,\n",
       " 'know': 442,\n",
       " 'months': 443,\n",
       " 'beta': 444,\n",
       " 'testing': 445,\n",
       " 'solid': 446,\n",
       " 'start': 447,\n",
       " 'keep': 448,\n",
       " 'improve': 449,\n",
       " 'Thanks': 450,\n",
       " 'Christa': 451,\n",
       " 'Will': 452,\n",
       " 'adding': 453,\n",
       " 'features': 454,\n",
       " 'overall': 455,\n",
       " 'upvotes': 456,\n",
       " 'article': 457,\n",
       " 'itself': 458,\n",
       " 'Also': 459,\n",
       " 'notification': 460,\n",
       " 'settings': 461,\n",
       " 'users': 462,\n",
       " 'Our': 463,\n",
       " 'aim': 464,\n",
       " 'opposite': 465,\n",
       " 'spirited': 466,\n",
       " 'free': 467,\n",
       " 'participate': 468,\n",
       " 'without': 469,\n",
       " 'fear': 470,\n",
       " 'harassment': 471,\n",
       " 'death': 472,\n",
       " 'threats': 473,\n",
       " 'Right': 474,\n",
       " 'voices': 475,\n",
       " 'due': 476,\n",
       " 'respectful': 477,\n",
       " 'positive': 478,\n",
       " 'There': 479,\n",
       " 'going': 480,\n",
       " 'prevent': 481,\n",
       " 'speak': 482,\n",
       " 'mind': 483,\n",
       " 'opinion': 484,\n",
       " 'treat': 485,\n",
       " 'respect': 486,\n",
       " 'civility': 487,\n",
       " 'dynamic': 488,\n",
       " 'interesting': 489,\n",
       " 'applaud': 490,\n",
       " 'efforts': 491,\n",
       " 'create': 492,\n",
       " 'new': 493,\n",
       " 'technology': 494,\n",
       " 'field': 495,\n",
       " 'Hoping': 496,\n",
       " 'thoughtful': 497,\n",
       " 'moving': 498,\n",
       " 'Why': 499,\n",
       " 'bother': 500,\n",
       " 'writing': 501,\n",
       " 'review': 502,\n",
       " 'devoid': 503,\n",
       " 'content': 504,\n",
       " 'understand': 505,\n",
       " 'hardly': 506,\n",
       " 'excuse': 507,\n",
       " 'Fred': 508,\n",
       " 'Armisen': 509,\n",
       " 'recorded': 510,\n",
       " 'radio': 511,\n",
       " 'Grand': 512,\n",
       " 'Theft': 513,\n",
       " 'Auto': 514,\n",
       " 'IV': 515,\n",
       " 'same': 516,\n",
       " 'number': 517,\n",
       " 'Yet': 518,\n",
       " 'call': 519,\n",
       " 'Muslims': 520,\n",
       " 'acts': 521,\n",
       " 'few': 522,\n",
       " 'pilloried': 523,\n",
       " '  ': 524,\n",
       " 'okay': 525,\n",
       " 'smear': 526,\n",
       " 'entire': 527,\n",
       " 'religion': 528,\n",
       " 'over': 529,\n",
       " 'idiots': 530,\n",
       " 'Or': 531,\n",
       " 'bash': 532,\n",
       " 'Christian': 533,\n",
       " 'sects': 534,\n",
       " 'upvoting': 535,\n",
       " 'articles': 536,\n",
       " 'Publisher': 537,\n",
       " 'could': 538,\n",
       " 'turn': 539,\n",
       " 'off': 540,\n",
       " 'ton': 541,\n",
       " 'following': 542,\n",
       " 'blocking': 543,\n",
       " 'bookmarking': 544,\n",
       " 'during': 545,\n",
       " 'process': 546,\n",
       " 'later': 547,\n",
       " 'find': 548,\n",
       " 'myself': 549,\n",
       " 'wanting': 550,\n",
       " 'reviewing': 551,\n",
       " 'conversations': 552,\n",
       " 'enthusiasm': 553,\n",
       " 'Melinda': 554,\n",
       " 'patience': 555,\n",
       " 'brand': 556,\n",
       " 'approach': 557,\n",
       " 'continue': 558,\n",
       " 'welcome': 559,\n",
       " 'suggestions': 560,\n",
       " 'feedback': 561,\n",
       " 'Troll': 562,\n",
       " 'since': 563,\n",
       " '2016': 564,\n",
       " 'bitch': 565,\n",
       " 'nuts': 566,\n",
       " 'Who': 567,\n",
       " 'book': 568,\n",
       " 'woman': 569,\n",
       " 'In': 570,\n",
       " 'Training': 571,\n",
       " 'Commenting': 572,\n",
       " 'sake': 573,\n",
       " 'rate': 574,\n",
       " 'our': 575,\n",
       " 'concept': 576,\n",
       " 'How': 577,\n",
       " 'plan': 578,\n",
       " 'monetize': 579,\n",
       " 'operation': 580,\n",
       " 'Pity': 581,\n",
       " 'menu': 582,\n",
       " 'lost': 583,\n",
       " 'vegan': 584,\n",
       " 'food': 585,\n",
       " 'Mash': 586,\n",
       " 'Tun': 587,\n",
       " 'favorite': 588,\n",
       " 'beer': 589,\n",
       " 'bar': 590,\n",
       " 'delicious': 591,\n",
       " 'tempeh': 592,\n",
       " 'stuff': 593,\n",
       " '\\n': 594,\n",
       " 'Excited': 595,\n",
       " 'staff': 596,\n",
       " 'still': 597,\n",
       " 'wait': 598,\n",
       " 'beers': 599,\n",
       " 'dozens': 600,\n",
       " 'just': 601,\n",
       " 'vote': 602,\n",
       " 'type': 603,\n",
       " 'voting': 604,\n",
       " 'completely': 605,\n",
       " 'insane-': 606,\n",
       " 'extra': 607,\n",
       " 'multiple': 608,\n",
       " 'non': 609,\n",
       " 'related': 610,\n",
       " 'installed': 611,\n",
       " 'oregonlive.com': 612,\n",
       " 'enough': 613,\n",
       " 'visitors': 614,\n",
       " 'established': 615,\n",
       " 'troll': 616,\n",
       " 'base': 617,\n",
       " 'service': 618,\n",
       " 'With': 619,\n",
       " 'little': 620,\n",
       " 'activity': 621,\n",
       " 'wweek': 622,\n",
       " 'discourage': 623,\n",
       " 'further': 624,\n",
       " 'growth': 625,\n",
       " 'project': 626,\n",
       " 'Signed': 627,\n",
       " 'shot': 628,\n",
       " '...': 629,\n",
       " 'luck': 630,\n",
       " 'enterprise': 631,\n",
       " 'disqus': 632,\n",
       " 'functionality': 633,\n",
       " 'obviously': 634,\n",
       " 'huge': 635,\n",
       " 'increase': 636,\n",
       " 'visitor': 637,\n",
       " 'count': 638,\n",
       " 'significantly': 639,\n",
       " 'moderator': 640,\n",
       " 'feature': 641,\n",
       " 'either': 642,\n",
       " 'never': 643,\n",
       " 'wanted': 644,\n",
       " 'pay': 645,\n",
       " 'undertake': 646,\n",
       " 'developed': 647,\n",
       " 'trust': 648,\n",
       " 'behalf': 649,\n",
       " 'volunteer': 650,\n",
       " 'basis': 651,\n",
       " 'YET': 652,\n",
       " 'ANOTHER': 653,\n",
       " 'BARACK': 654,\n",
       " 'OBAMA': 655,\n",
       " 'LIBERAL': 656,\n",
       " 'MEDIA': 657,\n",
       " 'CONSPIRACY': 658,\n",
       " 'BY': 659,\n",
       " 'THE': 660,\n",
       " 'THOUGHT': 661,\n",
       " 'CONTROL': 662,\n",
       " 'POLICE': 663,\n",
       " 'DIDENT': 664,\n",
       " 'SPEND': 665,\n",
       " '30': 666,\n",
       " 'YEARS': 667,\n",
       " 'MIXING': 668,\n",
       " 'CONCRETE': 669,\n",
       " 'TO': 670,\n",
       " 'LET': 671,\n",
       " 'AMERICA': 672,\n",
       " 'FALL': 673,\n",
       " 'COMMIES': 674,\n",
       " 'TAXES': 675,\n",
       " 'HERE': 676,\n",
       " 'ARE': 677,\n",
       " 'SO': 678,\n",
       " 'HIGH': 679,\n",
       " 'CANT': 680,\n",
       " 'EVEN': 681,\n",
       " 'AFFORD': 682,\n",
       " 'A': 683,\n",
       " 'KEYBOARD': 684,\n",
       " 'WITH': 685,\n",
       " 'WORKING': 686,\n",
       " 'CAPS': 687,\n",
       " 'LOCK': 688,\n",
       " 'PORTLAND': 689,\n",
       " 'HAS': 690,\n",
       " 'BEEN': 691,\n",
       " 'GOING': 692,\n",
       " 'DOWNHILL': 693,\n",
       " 'FOR': 694,\n",
       " 'NO': 695,\n",
       " 'WONDER': 696,\n",
       " 'TEA': 697,\n",
       " 'PARTY': 698,\n",
       " 'IS': 699,\n",
       " 'MAKING': 700,\n",
       " 'COMEBACK': 701,\n",
       " 'WHATS': 702,\n",
       " 'NEXT': 703,\n",
       " 'FLOURAIDE': 704,\n",
       " 'IN': 705,\n",
       " 'WATER': 706,\n",
       " 'SUPPLY': 707,\n",
       " 'seen': 708,\n",
       " 'kind': 709,\n",
       " 'mentioning': 710,\n",
       " 'interested': 711,\n",
       " 'where': 712,\n",
       " 'having': 713,\n",
       " 'problems': 714,\n",
       " 'Do': 715,\n",
       " 'link': 716,\n",
       " 'mentioned': 717,\n",
       " 'crazy': 718,\n",
       " 'illustration': 719,\n",
       " 'thought': 720,\n",
       " 'pitch': 721,\n",
       " 'everything': 722,\n",
       " 'yellow': 723,\n",
       " 'orange': 724,\n",
       " 'wmcelha': 725,\n",
       " 'exciting': 726,\n",
       " 'gluten': 727,\n",
       " 'options': 728,\n",
       " 'Buffalo': 729,\n",
       " 'tostones': 730,\n",
       " 'fun': 731,\n",
       " 'soy': 732,\n",
       " 'option': 733,\n",
       " 'mention': 734,\n",
       " 'killer': 735,\n",
       " 'veggie': 736,\n",
       " 'burger': 737,\n",
       " 'tacos': 738,\n",
       " 'nice': 739,\n",
       " 'salads': 740,\n",
       " 'Hope': 741,\n",
       " 'soon': 742,\n",
       " 'thank': 743,\n",
       " 'polluted': 744,\n",
       " 'far': 745,\n",
       " 'too': 746,\n",
       " 'lack': 747,\n",
       " 'moderation': 748,\n",
       " 'gave': 749,\n",
       " 'loud': 750,\n",
       " 'megaphone': 751,\n",
       " 'wing': 752,\n",
       " 'Probably': 753,\n",
       " 'consistently': 754,\n",
       " 'waste': 755,\n",
       " 'funds': 756,\n",
       " 'trendy': 757,\n",
       " 'projects': 758,\n",
       " 'green': 759,\n",
       " 'bike': 760,\n",
       " 'boxes': 761,\n",
       " '--': 762,\n",
       " 'cost': 763,\n",
       " 'fortune': 764,\n",
       " 'repainted': 765,\n",
       " 'years': 766,\n",
       " 'Oy': 767,\n",
       " 'set': 768,\n",
       " 'traffic': 769,\n",
       " 'slow': 770,\n",
       " 'algorithm': 771,\n",
       " 'takes': 772,\n",
       " 'until': 773,\n",
       " 'picks': 774,\n",
       " 'Because': 775,\n",
       " 'drive': 776,\n",
       " 'cars': 777,\n",
       " 'ones': 778,\n",
       " 'cause': 779,\n",
       " 'wear': 780,\n",
       " 'tear': 781,\n",
       " 'roads': 782,\n",
       " 'Pretty': 783,\n",
       " 'fair': 784,\n",
       " 'straightforward': 785,\n",
       " 'Affordable': 786,\n",
       " 'housing': 787,\n",
       " 'built': 788,\n",
       " 'pursuant': 789,\n",
       " 'tax': 790,\n",
       " '#': 791,\n",
       " '3': 792,\n",
       " 'immediately': 793,\n",
       " 'becomes': 794,\n",
       " 'affordable': 795,\n",
       " '2': 796,\n",
       " 'Brilliant': 797,\n",
       " 'Tried': 798,\n",
       " 'another': 799,\n",
       " 'post': 800,\n",
       " 'Having': 801,\n",
       " 'three': 802,\n",
       " 'own': 803,\n",
       " 'lead': 804,\n",
       " 'designer': 805,\n",
       " 'big': 806,\n",
       " 'Foucault': 807,\n",
       " 'fan': 808,\n",
       " 'Feels': 809,\n",
       " 'panopticomments': 810,\n",
       " 'suspect': 811,\n",
       " 'go': 812,\n",
       " 'back': 813,\n",
       " 'prefer': 814,\n",
       " 'ubiquity': 815,\n",
       " 'notifications': 816,\n",
       " 'may': 817,\n",
       " 'inadvertently': 818,\n",
       " 'given': 819,\n",
       " '2.0': 820,\n",
       " 'version': 821,\n",
       " '1': 822,\n",
       " 'Did': 823,\n",
       " 'totally': 824,\n",
       " 'win': 825,\n",
       " 'argument': 826,\n",
       " 'had': 827,\n",
       " 'trouble': 828,\n",
       " 'getting': 829,\n",
       " 'signed': 830,\n",
       " 'after': 831,\n",
       " 'brief': 832,\n",
       " 'email': 833,\n",
       " 'exchange': 834,\n",
       " 'folks': 835,\n",
       " 'appears': 836,\n",
       " 'If': 837,\n",
       " 'successful': 838,\n",
       " 'goal': 839,\n",
       " 'extremely': 840,\n",
       " 'pleased': 841,\n",
       " 'Are': 842,\n",
       " 'taxed': 843,\n",
       " 'state': 844,\n",
       " 'were': 845,\n",
       " 'bought': 846,\n",
       " 'ticket': 847,\n",
       " 'claim': 848,\n",
       " 'latter': 849,\n",
       " 'suppose': 850,\n",
       " 'theoretical': 851,\n",
       " 'lucky': 852,\n",
       " 'Oregonian': 853,\n",
       " 'decide': 854,\n",
       " 'whether': 855,\n",
       " 'worth': 856,\n",
       " '$': 857,\n",
       " '90': 858,\n",
       " 'm': 859,\n",
       " 'stuck': 860,\n",
       " 'hated': 861,\n",
       " 'Californian': 862,\n",
       " 'moniker': 863,\n",
       " 'letter': 864,\n",
       " 'campaign': 865,\n",
       " 'livestock': 866,\n",
       " 'Malheur': 867,\n",
       " 'Refuge': 868,\n",
       " 'started': 869,\n",
       " 'late': 870,\n",
       " '70': 871,\n",
       " 'phoned': 872,\n",
       " 'Nancy': 873,\n",
       " 'Denzel': 874,\n",
       " 'home': 875,\n",
       " '1979': 876,\n",
       " 'along': 877,\n",
       " 'friends': 878,\n",
       " 'hired': 879,\n",
       " 'bodyguard': 880,\n",
       " 'help': 881,\n",
       " 'protect': 882,\n",
       " 'N&D': 883,\n",
       " 'weekend': 884,\n",
       " 'Hammond': 885,\n",
       " 'ejection': 886,\n",
       " 'Ferguson': 887,\n",
       " 'Diamond': 888,\n",
       " 'Dance': 889,\n",
       " 'year': 890,\n",
       " 'before': 891,\n",
       " 'Jim': 892,\n",
       " 'D.': 893,\n",
       " 'For': 894,\n",
       " 'last': 895,\n",
       " '10': 896,\n",
       " 'days': 897,\n",
       " 'lots': 898,\n",
       " 'us': 899,\n",
       " 'wishing': 900,\n",
       " 'hear': 901,\n",
       " 'witty': 902,\n",
       " 'quips': 903,\n",
       " 'Bundycon': 904,\n",
       " 'occupation': 905,\n",
       " 'refuge': 906,\n",
       " 'hq': 907,\n",
       " 'Sacred': 908,\n",
       " 'Cows': 909,\n",
       " 'trough': 910,\n",
       " 'easy': 911,\n",
       " 'Amazon': 912,\n",
       " 'Read': 913,\n",
       " 'learn': 914,\n",
       " 'western': 915,\n",
       " 'abuses': 916,\n",
       " 'Mormons': 917,\n",
       " 'complicated': 918,\n",
       " 'relationship': 919,\n",
       " 'federal': 920,\n",
       " 'law': 921,\n",
       " 'Send': 922,\n",
       " 'STAT': 923,\n",
       " 'loosely': 924,\n",
       " 'announced': 925,\n",
       " 'tried': 926,\n",
       " 'demonstration': 927,\n",
       " 'wondering': 928,\n",
       " 'sign': 929,\n",
       " 'profiles': 930,\n",
       " 'uses': 931,\n",
       " 'profile': 932,\n",
       " 'use': 933,\n",
       " 'partner': 934,\n",
       " 'My': 935,\n",
       " 'smoother': 936,\n",
       " 'many': 937,\n",
       " 'sites': 938,\n",
       " 'overtaken': 939,\n",
       " 'first': 940,\n",
       " 'its': 941,\n",
       " 'intent': 942,\n",
       " 'town': 943,\n",
       " 'consider': 944,\n",
       " 'engagement': 945,\n",
       " 'rather': 946,\n",
       " 'driven': 947,\n",
       " 'click': 948,\n",
       " 'driver': 949,\n",
       " 'intended': 950,\n",
       " 'boost': 951,\n",
       " 'ad': 952,\n",
       " 'rates': 953,\n",
       " 'progress': 954,\n",
       " 'helping': 955,\n",
       " 'test': 956,\n",
       " 'software': 957,\n",
       " 'David': 958,\n",
       " 'To': 959,\n",
       " 'arching': 960,\n",
       " 'account': 961,\n",
       " 'manage': 962,\n",
       " 'different': 963,\n",
       " 'Of': 964,\n",
       " 'course': 965,\n",
       " 'information': 966,\n",
       " 'across': 967,\n",
       " 'decided': 968,\n",
       " 'best': 969,\n",
       " 'present': 970,\n",
       " 'yourself': 971,\n",
       " 'differently': 972,\n",
       " 'might': 973,\n",
       " 'example': 974,\n",
       " 'portland_hipster15': 975,\n",
       " 'Willamette': 976,\n",
       " 'Week': 977,\n",
       " 'MrWhiskers': 978,\n",
       " 'Cat': 979,\n",
       " 'Enthusiast': 980,\n",
       " 'Digest': 981,\n",
       " 'Enterprise': 982,\n",
       " 'licenses': 983,\n",
       " 'available': 984,\n",
       " 'tiered': 985,\n",
       " 'business': 986,\n",
       " 'plans': 987,\n",
       " 'very': 988,\n",
       " 'certainly': 989,\n",
       " 'journalism': 990,\n",
       " 'stretch': 991,\n",
       " 'imagination': 992,\n",
       " 'words': 993,\n",
       " 'metric': 994,\n",
       " 'chooses': 995,\n",
       " 'publish': 996,\n",
       " 'journalist': 997,\n",
       " 'means': 998,\n",
       " 'Brace': 999,\n",
       " 'Belden': 1000,\n",
       " ...}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
