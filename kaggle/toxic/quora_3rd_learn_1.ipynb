{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "import gensim\n",
    "import re\n",
    "import gc\n",
    "\n",
    "from keras.layers import Dense, Input, CuDNNLSTM, CuDNNGRU, Embedding, SpatialDropout1D\n",
    "from keras.layers import Bidirectional, GlobalMaxPooling1D, Concatenate \n",
    "from keras.optimizers import Adam\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "from keras.engine import InputSpec, Layer\n",
    "from keras import initializers, optimizers, layers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "import spacy\n",
    "from nltk.stem import PorterStemmer\n",
    "ps = PorterStemmer()\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "lc = LancasterStemmer()\n",
    "from nltk.stem import SnowballStemmer\n",
    "sb = SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell_model = gensim.models.KeyedVectors.load_word2vec_format('../input/quora-insincere-questions-classification/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = spell_model.index2word\n",
    "w_rank = {}\n",
    "for i, word in enumerate(words):\n",
    "    w_rank[word] = i\n",
    "WORDS = w_rank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del spell_model\n",
    "gc.collect()\n",
    "\n",
    "def words(text):\n",
    "    return re.findall(r'\\w+', text.lower()) \n",
    "    # ['https', 'docs', 'python', 'org']\n",
    "\n",
    "def P(word):\n",
    "    # Probability of \"word\"\n",
    "    return - WORDS.get(word, 0)\n",
    "\n",
    "def known(words):\n",
    "    # The subset of 'words' that appear in the dictionary of WORDS\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    # All edits that are one edit away from 'word'\n",
    "    letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits = [(word[:i], word[i:]) for i in range(len(word) + 1)]\n",
    "    # [('', 'splits'), ('s', 'plits'),...('splits', '')]\n",
    "    deletes = [L + R[1:] for L, R in splits if R]\n",
    "    # ['plits', 'slits', 'spits', 'splts', 'splis', 'split']\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n",
    "    # ['pslits', 'slpits', 'spilts', 'spltis', 'splist']\n",
    "    replaces = [L + c + R[1:] for L, R in splits if R for c in letters]\n",
    "    # ['aplits', 'bplits',...'splitx', 'splity', 'splitz']\n",
    "    inserts = [L + c + R for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word):\n",
    "    # All edits that are two edits away from 'word'\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))    \n",
    "    \n",
    "def candidates(word):\n",
    "    # Generate possible spelling corrections for word\n",
    "    return (known([word]) or known(edits1(word)) or [word])\n",
    "\n",
    "def correction(word):\n",
    "    # Most probable spelling correction for word\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def singlify(word):\n",
    "    return ''.join([letter for i, letter in enumerate(word) if i==0 or letter!=word[i-1]])\n",
    "    # 'apple' => 'aple'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(word_dict, lemma_dict):\n",
    "    EMBEDDING_FILE = '../input/quora-insincere-questions-classification/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "    def get_coefs(word, *arr):\n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "    # embedding_index = {',':300d vec, 'this':300d vec.....}\n",
    "    embed_size = 300\n",
    "    nb_words = len(word_dict) + 1\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size), dtype=np.float32)\n",
    "    unknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1\n",
    "    for key in tqdm(word_dict):\n",
    "        # word_dict = {'key': 'index'}\n",
    "        word = key\n",
    "        embedding_vector = embeddings_index.get(word) # 返回word对应的300d vec\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.lower()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.upper()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.capitalize()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = ps.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = lc.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = sb.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = lemma_dict[key]\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        if len(key) > 1:\n",
    "            word = correction(key)\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[word_dict[key]] = embedding_vector\n",
    "                continue\n",
    "        embedding_matrix[word_dict[key]] = unknown_vector\n",
    "    return embedding_matrix, nb_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_fasttext(word_dict, lemma_dict):\n",
    "    EMBEDDING_FILE = '../input/quora-insincere-questions-classification/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "    def get_coefs(word, *arr):\n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n",
    "    # embedding_index = {',':300d vec, 'this':300d vec.....}\n",
    "    embed_size = 300\n",
    "    nb_words = len(word_dict) + 1\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size), dtype=np.float32)\n",
    "    unknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1\n",
    "    for key in tqdm(word_dict):\n",
    "        # word_dict = {'key': 'index'}\n",
    "        word = key\n",
    "        embedding_vector = embeddings_index.get(word) # 返回word对应的300d vec\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.lower()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.upper()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.capitalize()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = ps.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = lc.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = sb.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = lemma_dict[key]\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        if len(key) > 1:\n",
    "            word = correction(key)\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[word_dict[key]] = embedding_vector\n",
    "                continue\n",
    "        embedding_matrix[word_dict[key]] = unknown_vector\n",
    "    return embedding_matrix, nb_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_para(word_dict, lemma_dict):\n",
    "    EMBEDDING_FILE = '../input/quora-insincere-questions-classification/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "    def get_coefs(word, *arr):\n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding='utf-8', errors='ignore') if len(o)>100)\n",
    "    # embedding_index = {',':300d vec, 'this':300d vec.....}\n",
    "    embed_size = 300\n",
    "    nb_words = len(word_dict) + 1\n",
    "    embedding_matrix = np.zeros((nb_words, embed_size), dtype=np.float32)\n",
    "    unknown_vector = np.zeros((embed_size,), dtype=np.float32) - 1\n",
    "    for key in tqdm(word_dict):\n",
    "        # word_dict = {'key': 'index'}\n",
    "        word = key\n",
    "        embedding_vector = embeddings_index.get(word) # 返回word对应的300d vec\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.lower()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.upper()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = key.capitalize()\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = ps.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = lc.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = sb.stem(key)\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        word = lemma_dict[key]\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[word_dict[key]] = embedding_vector\n",
    "            continue\n",
    "        if len(key) > 1:\n",
    "            word = correction(key)\n",
    "            embedding_vector = embeddings_index.get(word)\n",
    "            if embedding_vector is not None:\n",
    "                embedding_matrix[word_dict[key]] = embedding_vector\n",
    "                continue\n",
    "        embedding_matrix[word_dict[key]] = unknown_vector\n",
    "    return embedding_matrix, nb_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(embedding_matrix, nb_words, embedding_size=300):\n",
    "    inp = Input(shape=(max_length,))\n",
    "    x = Embedding(nb_words, embedding_size, weights=[embedding_matrix], trainable=False)(inp)\n",
    "    x = SpatialDropout1D(0.3)(x)\n",
    "    x1 = Bidirectional(CuDNNLSTM(256, return_sequences=True))(x)\n",
    "    x2 = Bidirectional(CuDNNGRU(128, return_sequences=True))(x1)\n",
    "    max_pool1 = GlobalMaxPooling1D()(x1)\n",
    "    max_pool2 = GlobalMaxPooling1D()(x2)\n",
    "    conc = Concatenate()([max_pool1, max_pool2])\n",
    "    predictions = Dense(1, activation='sigmoid')(conc)\n",
    "    model = Model(inputs=inp, outputs=predictions)\n",
    "    adam = optimizers.Adam(lr=learning_rate)\n",
    "    model.compile(optimizer=adam, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
    "                       \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
    "                       \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
    "                       \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \n",
    "                       \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \n",
    "                       \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \n",
    "                       \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \n",
    "                       \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
    "                       \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\n",
    "                       \"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \n",
    "                       \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \n",
    "                       \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
    "                       \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \n",
    "                       \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \n",
    "                       \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n",
    "                       \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \n",
    "                       \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \n",
    "                       \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \n",
    "                       \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \n",
    "                       \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \n",
    "                       \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n",
    "                       \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \n",
    "                       \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\n",
    "                       \"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \n",
    "                       \"you're\": \"you are\", \"you've\": \"you have\" }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replace_with_fuck = ['4r5e',  '5h1t', '5hit', 'ass-fucker', 'assfucker', 'assfukka', 'asswhole', 'a_s_s', \n",
    "                     'b!tch', 'b17ch', 'blow job', 'boiolas', 'bollok', 'boooobs', 'booooobs', 'booooooobs',\n",
    "                     'bunny fucker', 'buttmuch', 'c0cksucker', 'carpet muncher', 'cl1t', 'cockface', 'cockmunch',\n",
    "                     'cockmuncher', 'cocksuka', 'cocksukka', 'cokmuncher', 'coksucka', 'cunillingus', 'cuntlick',\n",
    "                     'cuntlicker', 'cuntlicking', 'cyalis', 'cyberfuc', 'cyberfuck', 'cyberfucked', 'cyberfucker',\n",
    "                     'cyberfuckers', 'cyberfucking', 'dirsa', 'dlck', 'dog-fucker', 'donkeyribber', 'ejaculatings',\n",
    "                     'ejakulate', 'f u c k', 'f u c k e r', 'f4nny', 'faggitt', 'faggs', 'fannyflaps', \n",
    "                     'fannyfucker', 'fanyy', 'fingerfucker', 'fingerfuckers', 'fingerfucks', 'fistfuck', 'fistfucked',\n",
    "                     'fistfucker', 'fistfuckers', 'fistfucking', 'fistfuckings', 'fistfucks', 'fuckingshitmotherfucker',\n",
    "                     'fuckwhit', 'fudge packer', 'fudgepacker', 'fukwhit', 'fukwit', 'fux0r', 'f_u_c_k', 'god-dam',\n",
    "                     'kawk', 'knobead', 'knobed', 'knobend', 'knobjocky', 'knobjokey', 'kondum', 'kondums', 'kummer',\n",
    "                     'kumming', 'kums', 'kunilingus', 'l3itch', 'm0f0', 'm0fo', 'm45terbate', 'ma5terb8', 'ma5terbate',\n",
    "                     'master-bate', 'masterb8', 'masterbat3', 'masterbations', 'mof0', 'mothafuck', 'mothafuckaz',\n",
    "                     'mothafucked', 'mothafucking', 'mothafuckings', 'mothafucks', 'mother fucker', 'motherfucked',\n",
    "                     'motherfuckings', 'motherfuckka', 'motherfucks', 'muthafecker', 'muthafuckker', 'n1gga', 'n1gger',\n",
    "                     'nigg3r', 'nigg4h', 'nob jokey', 'nobjocky', 'nobjokey', 'penisfucker', 'phuked', 'phuking',\n",
    "                     'phukked', 'phukking', 'phuks', 'phuq', 'pigfucker', 'pimpis', 'pissflaps', 'rimjaw', 's hit',\n",
    "                     'scroat', 'sh!t', 'shitdick', 'shitfull', 'shitings', 'shittings', 's_h_i_t', 't1tt1e5', \n",
    "                     't1tties', 'teez', 'tittie5', 'tittiefucker', 'tittywank', 'tw4t', 'twathead', 'twunter',\n",
    "                     'v14gra', 'v1gra', 'w00se', 'whoar']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text):\n",
    "       \n",
    "    text = ' '.join([contraction_mapping[t] if t in contraction_mapping else t for t in text.split(\" \")])        \n",
    "    text = ' '.join(['fuck' if t in replace_with_fuck else t for t in text.split(\" \")])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Loading data...\")\n",
    "train = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/train.csv').fillna(' ')\n",
    "test = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/test.csv').fillna(' ')\n",
    "train_text = train['comment_text'].apply(lambda x: clean(x))\n",
    "test_text = test['comment_text'].apply(lambda x: clean(x))\n",
    "text_list = pd.concat([train_text, test_text])\n",
    "y = train['target'].values\n",
    "num_train_data = y.shape[0]\n",
    "print('--- %s seconds ---' % (time.time() - start_time))\n",
    "\n",
    "del(train, test, train_text, test_text)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Spacy NLP...')\n",
    "nlp = spacy.load('en_core_web_lg', disable=['parser', 'ner', 'tagger'])\n",
    "nlp.vocab.add_flag(lambda s: s.lower() in spacy.lang.en.stop_words.STOP_WORDS, spacy.attrs.IS_STOP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# docs = [token1, token2...]\n",
    "# token.text          内容\n",
    "# token.idx\n",
    "# token.lemma_        词元\n",
    "# token.is_punct      标点\n",
    "# token.is_space      空格\n",
    "# token.shape_        正字特征 如Xxx, xxxx, dd\n",
    "# token.pos_          粗粒度的词性， 如NOUN名词， PUNCT标点\n",
    "# token.tag_          细粒度的词性， 如NN， VBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_dict = {}\n",
    "word_index = 1\n",
    "lemma_dict = {}\n",
    "docs = nlp.pipe(text_list, n_threads=3)\n",
    "for doc in tqdm(docs, total=1902194):\n",
    "    word_seq = []\n",
    "    for token in doc:\n",
    "        if token.text not in word_dict:\n",
    "            word_dict[token.text] = word_index\n",
    "            word_index += 1\n",
    "            lemma_dict[token.text] = token.lemma_\n",
    "\n",
    "# word_dict = {'The': 1, 'is': 2....}\n",
    "# lemma_dict = {'The': 'the', 'is': 'is'...}\n",
    "# word_seq = [1, 2, ...]\n",
    "# word_sequences = [[1, 2, ...], [...], ...]\n",
    "# 1902194it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Loading embedding matrix...\")\n",
    "embedding_matrix_glove, nb_words = load_glove(word_dict, lemma_dict)\n",
    "embedding_matrix_fasttext, nb_words = load_fasttext(word_dict, lemma_dict)\n",
    "embedding_matrix = np.concatenate((embedding_matrix_glove, embedding_matrix_fasttext), axis=1)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "# embedding matrix 536300 * 600"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zeros = np.zeros((600,)) - 1\n",
    "unknown_words = []\n",
    "for i in range(len(embedding_matrix) - 1):\n",
    "    if (embedding_matrix[i] == zeros).all():\n",
    "        unknown_words.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_word_dict = {}\n",
    "index = 1\n",
    "for word in word_dict:\n",
    "    if word_dict[word] not in unknown_words:\n",
    "        new_word_dict[word] = index\n",
    "        index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(word_dict, embedding_matrix_glove, embedding_matrix_fasttext, embedding_matrix, nb_words)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(\"Loading embedding matrix...\")\n",
    "embedding_matrix_glove, nb_words = load_glove(new_word_dict, lemma_dict)\n",
    "embedding_matrix_fasttext, nb_words = load_fasttext(new_word_dict, lemma_dict)\n",
    "embedding_matrix = np.concatenate((embedding_matrix_glove, embedding_matrix_fasttext), axis=1)\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "word_sequences = []\n",
    "word_index = 1\n",
    "docs = nlp.pipe(text_list, n_threads=3)\n",
    "for doc in tqdm(docs, total=1902194):\n",
    "    word_seq = []\n",
    "    for token in doc:\n",
    "        if token.text in new_word_dict:\n",
    "            word_seq.append(new_word_dict[token.text])\n",
    "        else:\n",
    "            word_seq.append(0)\n",
    "    word_sequences.append(word_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_sequences = word_sequences[:num_train_data]\n",
    "test_word_sequences = word_sequences[num_train_data:]\n",
    "del(docs, nlp, word_sequences)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 100\n",
    "embedding_size = 600\n",
    "learning_rate = 0.001\n",
    "batch_size = 512\n",
    "num_epoch = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_word_sequences = pad_sequences(train_word_sequences, maxlen=max_length, padding='post')\n",
    "test_word_sequences = pad_sequences(test_word_sequences, maxlen=max_length, padding='post')\n",
    "print(train_word_sequences[0])\n",
    "print(test_word_sequences[0])\n",
    "pred_prob = np.zeros((len(test_word_sequences),), dtype=np.float32)\n",
    "\n",
    "del(text_list)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del(unknown_words, word, word_seq, words, zeros)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Start training ...')\n",
    "model = build_model(embedding_matrix, nb_words, embedding_size)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pynvml\n",
    "pynvml.nvmlInit()\n",
    "# 这里的1是GPU id\n",
    "handle = pynvml.nvmlDeviceGetHandleByIndex(0)\n",
    "meminfo = pynvml.nvmlDeviceGetMemoryInfo(handle)\n",
    "print(meminfo.total/1024/1024)\n",
    "print(meminfo.used/1024/1024)\n",
    "print(meminfo.free/1024/1024)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_word_sequences, y, batch_size=batch_size, epochs=num_epoch-1)\n",
    "pred_prob += 0.15*np.squeeze(model.predict(test_word_sequences, batch_size=batch_size, verbose=2))\n",
    "model.fit(train_word_sequences, y, batch_size=batch_size, epochs=1)\n",
    "pred_prob += 0.35*np.squeeze(model.predict(test_word_sequences, batch_size=batch_size, verbose=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del model, embedding_matrix_fasttext, embedding_matrix\n",
    "gc.collect()\n",
    "K.clear_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_matrix_para, nb_words = load_para(new_word_dict, lemma_dict)\n",
    "embedding_matrix = np.concatenate((embedding_matrix_glove, embedding_matrix_para), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(embedding_matrix, nb_words, embedding_size)\n",
    "model.fit(train_word_sequences, y, batch_size=batch_size, epochs=num_epoch-1, verbose=2)\n",
    "pred_prob += 0.15*np.squeeze(model.predict(test_word_sequences, batch_size=batch_size, verbose=2))\n",
    "model.fit(train_word_sequences, y, batch_size=batch_size, epochs=1, verbose=2)\n",
    "pred_prob += 0.35*np.squeeze(model.predict(test_word_sequences, batch_size=batch_size, verbose=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission = pd.read_csv('../input/jigsaw-unintended-bias-in-toxicity-classification/sample_submission.csv', index_col='id')\n",
    "submission['prediction'] = pred_prob\n",
    "# submission.reset_index(drop=False, inplace=True)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# class AttentionWeightedAverage(Layer):\n",
    "#     '''\n",
    "#     Compute a weighted average of the different channels accross timesteps.\n",
    "#     Uses 1 parameter pr. channel to compute the attention value for a single timestep.\n",
    "#     '''\n",
    "    \n",
    "#     def __init__(self, return_attention=False, **kwargs):\n",
    "#         self.init = initialiers.get('uniform')\n",
    "#         self.supports_masking = True\n",
    "#         self.return_attention = return_attention\n",
    "#         super(AttentionWeightedAverage, self).__init__(**kwargs)\n",
    "        \n",
    "#     def build(self, input_shape):\n",
    "#         self.input_spec = [InputSpec(ndim=3)]\n",
    "#         assert len(input_spec) == 3\n",
    "        \n",
    "#         self.W = self.add_weight(shape=(input_shape[2], 1), name='{}_W'.format(self.name),\n",
    "#                                  initailizer=self.init)\n",
    "#         self.trainable_weights = [self.W]\n",
    "#         super(AttentionWeightedAverage, self).build(input_shape)\n",
    "        \n",
    "#     def call(self, x, mask=None):\n",
    "#         logits = K.dot(x, self.W)\n",
    "#         x_shape = K.shape(x)\n",
    "#         logits = K.reshape(logits, (x_shape[0], x_shape[1]))\n",
    "#         ai = K.exp(logits - K.max(logits, axis=-1, keepdims=True))\n",
    "        \n",
    "#         # masked timesteps have zero weight\n",
    "#         if mask is not None:\n",
    "#             mask = K.cast(mask, K.floatx())\n",
    "#             ai = ai * mask\n",
    "            \n",
    "#         att_weights = ai / (K.sum(ai, axis=1, keepdims=True) + K.epsilon())\n",
    "#         weighted_input = x * K.expand_dims(att_weights)\n",
    "#         result = K.sum(weighted_input, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
