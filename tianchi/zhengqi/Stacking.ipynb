{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.svm import NuSVR\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import warnings \n",
    "from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "warnings.filterwarnings('ignore')\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>V0</th>\n",
       "      <th>V1</th>\n",
       "      <th>V2</th>\n",
       "      <th>V3</th>\n",
       "      <th>V4</th>\n",
       "      <th>V5</th>\n",
       "      <th>V6</th>\n",
       "      <th>V7</th>\n",
       "      <th>V8</th>\n",
       "      <th>V9</th>\n",
       "      <th>...</th>\n",
       "      <th>V29</th>\n",
       "      <th>V30</th>\n",
       "      <th>V31</th>\n",
       "      <th>V32</th>\n",
       "      <th>V33</th>\n",
       "      <th>V34</th>\n",
       "      <th>V35</th>\n",
       "      <th>V36</th>\n",
       "      <th>V37</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.566</td>\n",
       "      <td>0.016</td>\n",
       "      <td>-0.143</td>\n",
       "      <td>0.407</td>\n",
       "      <td>0.452</td>\n",
       "      <td>-0.901</td>\n",
       "      <td>-1.812</td>\n",
       "      <td>-2.36</td>\n",
       "      <td>-0.436</td>\n",
       "      <td>-2.114</td>\n",
       "      <td>...</td>\n",
       "      <td>0.136</td>\n",
       "      <td>0.109</td>\n",
       "      <td>-0.615</td>\n",
       "      <td>0.327</td>\n",
       "      <td>-4.627</td>\n",
       "      <td>-4.789</td>\n",
       "      <td>-5.101</td>\n",
       "      <td>-2.608</td>\n",
       "      <td>-3.508</td>\n",
       "      <td>0.175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.968</td>\n",
       "      <td>0.437</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.566</td>\n",
       "      <td>0.194</td>\n",
       "      <td>-0.893</td>\n",
       "      <td>-1.566</td>\n",
       "      <td>-2.36</td>\n",
       "      <td>0.332</td>\n",
       "      <td>-2.114</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.128</td>\n",
       "      <td>0.124</td>\n",
       "      <td>0.032</td>\n",
       "      <td>0.600</td>\n",
       "      <td>-0.843</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.364</td>\n",
       "      <td>-0.335</td>\n",
       "      <td>-0.730</td>\n",
       "      <td>0.676</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.013</td>\n",
       "      <td>0.568</td>\n",
       "      <td>0.235</td>\n",
       "      <td>0.370</td>\n",
       "      <td>0.112</td>\n",
       "      <td>-0.797</td>\n",
       "      <td>-1.367</td>\n",
       "      <td>-2.36</td>\n",
       "      <td>0.396</td>\n",
       "      <td>-2.114</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.361</td>\n",
       "      <td>0.277</td>\n",
       "      <td>-0.116</td>\n",
       "      <td>-0.843</td>\n",
       "      <td>0.160</td>\n",
       "      <td>0.364</td>\n",
       "      <td>0.765</td>\n",
       "      <td>-0.589</td>\n",
       "      <td>0.633</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      V0     V1     V2     V3     V4     V5     V6    V7     V8     V9  \\\n",
       "0  0.566  0.016 -0.143  0.407  0.452 -0.901 -1.812 -2.36 -0.436 -2.114   \n",
       "1  0.968  0.437  0.066  0.566  0.194 -0.893 -1.566 -2.36  0.332 -2.114   \n",
       "2  1.013  0.568  0.235  0.370  0.112 -0.797 -1.367 -2.36  0.396 -2.114   \n",
       "\n",
       "    ...      V29    V30    V31    V32    V33    V34    V35    V36    V37  \\\n",
       "0   ...    0.136  0.109 -0.615  0.327 -4.627 -4.789 -5.101 -2.608 -3.508   \n",
       "1   ...   -0.128  0.124  0.032  0.600 -0.843  0.160  0.364 -0.335 -0.730   \n",
       "2   ...   -0.009  0.361  0.277 -0.116 -0.843  0.160  0.364  0.765 -0.589   \n",
       "\n",
       "   target  \n",
       "0   0.175  \n",
       "1   0.676  \n",
       "2   0.633  \n",
       "\n",
       "[3 rows x 39 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load in the train and test datasets\n",
    "train_data = pd.read_csv('zhengqi_train.txt', sep='\\t')\n",
    "test_data = pd.read_csv('zhengqi_test.txt', sep='\\t')\n",
    "\n",
    "train_data.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train_data.drop(\"target\", axis=1)\n",
    "y = train_data.target\n",
    "# X.drop([\"V5\",\"V9\",\"V11\",\"V17\",\"V22\",\"V28\"],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "X_all = pd.concat([X, test_data], axis=0)\n",
    "mmscaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_all = pd.DataFrame(mmscaler.fit_transform(X_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_std = pd.DataFrame(columns=['feature', 'std'])\n",
    "X_std['feature'] = X_all.columns\n",
    "X_std['std'] = X_all.var().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>0.018629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>0.021879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>0.027706</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>33</td>\n",
       "      <td>0.039279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>34</td>\n",
       "      <td>0.040820</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>35</td>\n",
       "      <td>0.041646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>31</td>\n",
       "      <td>0.042416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>0.044174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.045309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>0.050261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>0.055795</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>0.060806</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>0.062127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>36</td>\n",
       "      <td>0.064980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>0.065763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>30</td>\n",
       "      <td>0.068420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.069109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>0.070289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>0.070307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>0.071263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>0.071913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>0.071915</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>0.072773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>0.076797</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>0.078586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.079105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>0.079220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.083172</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>0.089803</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>37</td>\n",
       "      <td>0.090428</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.092978</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>32</td>\n",
       "      <td>0.096103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>0.106962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>0.107187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>0.119094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>0.135964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>0.139332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>0.281987</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    feature       std\n",
       "9         9  0.018629\n",
       "21       21  0.021879\n",
       "25       25  0.027706\n",
       "33       33  0.039279\n",
       "34       34  0.040820\n",
       "35       35  0.041646\n",
       "31       31  0.042416\n",
       "12       12  0.044174\n",
       "4         4  0.045309\n",
       "16       16  0.050261\n",
       "8         8  0.055795\n",
       "26       26  0.060806\n",
       "18       18  0.062127\n",
       "36       36  0.064980\n",
       "27       27  0.065763\n",
       "30       30  0.068420\n",
       "1         1  0.069109\n",
       "6         6  0.070289\n",
       "7         7  0.070307\n",
       "29       29  0.071263\n",
       "20       20  0.071913\n",
       "23       23  0.071915\n",
       "10       10  0.072773\n",
       "15       15  0.076797\n",
       "19       19  0.078586\n",
       "2         2  0.079105\n",
       "28       28  0.079220\n",
       "0         0  0.083172\n",
       "13       13  0.089803\n",
       "37       37  0.090428\n",
       "3         3  0.092978\n",
       "32       32  0.096103\n",
       "22       22  0.106962\n",
       "11       11  0.107187\n",
       "17       17  0.119094\n",
       "14       14  0.135964\n",
       "5         5  0.139332\n",
       "24       24  0.281987"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_std.sort_values(by='std')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=30)\n",
    "pca.fit(X_all)\n",
    "X = pd.DataFrame(pca.transform(X))\n",
    "test_data = pd.DataFrame(pca.transform(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2888.000000</td>\n",
       "      <td>2888.000000</td>\n",
       "      <td>2888.000000</td>\n",
       "      <td>2888.000000</td>\n",
       "      <td>2888.000000</td>\n",
       "      <td>2888.000000</td>\n",
       "      <td>2888.000000</td>\n",
       "      <td>2888.000000</td>\n",
       "      <td>2888.000000</td>\n",
       "      <td>2888.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2888.000000</td>\n",
       "      <td>2888.000000</td>\n",
       "      <td>2888.000000</td>\n",
       "      <td>2888.000000</td>\n",
       "      <td>2888.000000</td>\n",
       "      <td>2888.000000</td>\n",
       "      <td>2888.000000</td>\n",
       "      <td>2888.000000</td>\n",
       "      <td>2888.000000</td>\n",
       "      <td>2888.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.457562</td>\n",
       "      <td>0.051419</td>\n",
       "      <td>-0.347705</td>\n",
       "      <td>0.403201</td>\n",
       "      <td>0.329335</td>\n",
       "      <td>0.651832</td>\n",
       "      <td>-0.095726</td>\n",
       "      <td>0.848349</td>\n",
       "      <td>0.049792</td>\n",
       "      <td>0.527962</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063039</td>\n",
       "      <td>0.022400</td>\n",
       "      <td>0.102345</td>\n",
       "      <td>0.023113</td>\n",
       "      <td>0.008794</td>\n",
       "      <td>0.411535</td>\n",
       "      <td>0.093395</td>\n",
       "      <td>0.530517</td>\n",
       "      <td>-0.010201</td>\n",
       "      <td>0.367033</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.255421</td>\n",
       "      <td>1.980646</td>\n",
       "      <td>1.411745</td>\n",
       "      <td>1.640513</td>\n",
       "      <td>1.329481</td>\n",
       "      <td>1.414670</td>\n",
       "      <td>1.198888</td>\n",
       "      <td>0.994488</td>\n",
       "      <td>1.130644</td>\n",
       "      <td>0.946794</td>\n",
       "      <td>...</td>\n",
       "      <td>0.545031</td>\n",
       "      <td>0.493986</td>\n",
       "      <td>0.476030</td>\n",
       "      <td>0.618824</td>\n",
       "      <td>0.539555</td>\n",
       "      <td>0.471489</td>\n",
       "      <td>0.490747</td>\n",
       "      <td>0.538645</td>\n",
       "      <td>0.427683</td>\n",
       "      <td>0.461222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-4.122242</td>\n",
       "      <td>-5.460184</td>\n",
       "      <td>-4.206149</td>\n",
       "      <td>-5.646539</td>\n",
       "      <td>-3.652229</td>\n",
       "      <td>-4.045046</td>\n",
       "      <td>-4.942079</td>\n",
       "      <td>-2.626629</td>\n",
       "      <td>-4.756825</td>\n",
       "      <td>-2.609794</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.002206</td>\n",
       "      <td>-2.085738</td>\n",
       "      <td>-1.776918</td>\n",
       "      <td>-3.475335</td>\n",
       "      <td>-4.186476</td>\n",
       "      <td>-2.146308</td>\n",
       "      <td>-2.385539</td>\n",
       "      <td>-1.515362</td>\n",
       "      <td>-2.415485</td>\n",
       "      <td>-1.128047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.093009</td>\n",
       "      <td>-1.358489</td>\n",
       "      <td>-1.338777</td>\n",
       "      <td>-0.659852</td>\n",
       "      <td>-0.404309</td>\n",
       "      <td>-0.281147</td>\n",
       "      <td>-0.933199</td>\n",
       "      <td>0.233390</td>\n",
       "      <td>-0.678590</td>\n",
       "      <td>-0.137256</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.400804</td>\n",
       "      <td>-0.277867</td>\n",
       "      <td>-0.193642</td>\n",
       "      <td>-0.326507</td>\n",
       "      <td>-0.267159</td>\n",
       "      <td>0.135646</td>\n",
       "      <td>-0.176378</td>\n",
       "      <td>0.212343</td>\n",
       "      <td>-0.249845</td>\n",
       "      <td>0.084062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.000498</td>\n",
       "      <td>0.120956</td>\n",
       "      <td>-0.307531</td>\n",
       "      <td>0.145601</td>\n",
       "      <td>0.202916</td>\n",
       "      <td>0.597849</td>\n",
       "      <td>-0.090009</td>\n",
       "      <td>0.805541</td>\n",
       "      <td>0.034567</td>\n",
       "      <td>0.491874</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.068542</td>\n",
       "      <td>0.035617</td>\n",
       "      <td>0.092422</td>\n",
       "      <td>0.001038</td>\n",
       "      <td>0.011564</td>\n",
       "      <td>0.412186</td>\n",
       "      <td>0.072509</td>\n",
       "      <td>0.493245</td>\n",
       "      <td>-0.001625</td>\n",
       "      <td>0.308030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.531377</td>\n",
       "      <td>1.367773</td>\n",
       "      <td>0.601080</td>\n",
       "      <td>1.263737</td>\n",
       "      <td>0.815569</td>\n",
       "      <td>1.544217</td>\n",
       "      <td>0.728510</td>\n",
       "      <td>1.440870</td>\n",
       "      <td>0.692939</td>\n",
       "      <td>1.155083</td>\n",
       "      <td>...</td>\n",
       "      <td>0.250164</td>\n",
       "      <td>0.334688</td>\n",
       "      <td>0.402501</td>\n",
       "      <td>0.404582</td>\n",
       "      <td>0.300814</td>\n",
       "      <td>0.680790</td>\n",
       "      <td>0.322682</td>\n",
       "      <td>0.855335</td>\n",
       "      <td>0.255257</td>\n",
       "      <td>0.581019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>11.953057</td>\n",
       "      <td>7.914165</td>\n",
       "      <td>4.621223</td>\n",
       "      <td>9.318196</td>\n",
       "      <td>10.987218</td>\n",
       "      <td>7.913680</td>\n",
       "      <td>4.481226</td>\n",
       "      <td>5.310851</td>\n",
       "      <td>4.743251</td>\n",
       "      <td>4.723870</td>\n",
       "      <td>...</td>\n",
       "      <td>3.093192</td>\n",
       "      <td>1.990407</td>\n",
       "      <td>3.580457</td>\n",
       "      <td>1.978566</td>\n",
       "      <td>2.705869</td>\n",
       "      <td>4.245786</td>\n",
       "      <td>3.243924</td>\n",
       "      <td>5.873445</td>\n",
       "      <td>3.663861</td>\n",
       "      <td>5.021393</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0            1            2            3            4   \\\n",
       "count  2888.000000  2888.000000  2888.000000  2888.000000  2888.000000   \n",
       "mean      0.457562     0.051419    -0.347705     0.403201     0.329335   \n",
       "std       2.255421     1.980646     1.411745     1.640513     1.329481   \n",
       "min      -4.122242    -5.460184    -4.206149    -5.646539    -3.652229   \n",
       "25%      -1.093009    -1.358489    -1.338777    -0.659852    -0.404309   \n",
       "50%      -0.000498     0.120956    -0.307531     0.145601     0.202916   \n",
       "75%       1.531377     1.367773     0.601080     1.263737     0.815569   \n",
       "max      11.953057     7.914165     4.621223     9.318196    10.987218   \n",
       "\n",
       "                5            6            7            8            9   \\\n",
       "count  2888.000000  2888.000000  2888.000000  2888.000000  2888.000000   \n",
       "mean      0.651832    -0.095726     0.848349     0.049792     0.527962   \n",
       "std       1.414670     1.198888     0.994488     1.130644     0.946794   \n",
       "min      -4.045046    -4.942079    -2.626629    -4.756825    -2.609794   \n",
       "25%      -0.281147    -0.933199     0.233390    -0.678590    -0.137256   \n",
       "50%       0.597849    -0.090009     0.805541     0.034567     0.491874   \n",
       "75%       1.544217     0.728510     1.440870     0.692939     1.155083   \n",
       "max       7.913680     4.481226     5.310851     4.743251     4.723870   \n",
       "\n",
       "          ...                20           21           22           23  \\\n",
       "count     ...       2888.000000  2888.000000  2888.000000  2888.000000   \n",
       "mean      ...         -0.063039     0.022400     0.102345     0.023113   \n",
       "std       ...          0.545031     0.493986     0.476030     0.618824   \n",
       "min       ...         -2.002206    -2.085738    -1.776918    -3.475335   \n",
       "25%       ...         -0.400804    -0.277867    -0.193642    -0.326507   \n",
       "50%       ...         -0.068542     0.035617     0.092422     0.001038   \n",
       "75%       ...          0.250164     0.334688     0.402501     0.404582   \n",
       "max       ...          3.093192     1.990407     3.580457     1.978566   \n",
       "\n",
       "                24           25           26           27           28  \\\n",
       "count  2888.000000  2888.000000  2888.000000  2888.000000  2888.000000   \n",
       "mean      0.008794     0.411535     0.093395     0.530517    -0.010201   \n",
       "std       0.539555     0.471489     0.490747     0.538645     0.427683   \n",
       "min      -4.186476    -2.146308    -2.385539    -1.515362    -2.415485   \n",
       "25%      -0.267159     0.135646    -0.176378     0.212343    -0.249845   \n",
       "50%       0.011564     0.412186     0.072509     0.493245    -0.001625   \n",
       "75%       0.300814     0.680790     0.322682     0.855335     0.255257   \n",
       "max       2.705869     4.245786     3.243924     5.873445     3.663861   \n",
       "\n",
       "                29  \n",
       "count  2888.000000  \n",
       "mean      0.367033  \n",
       "std       0.461222  \n",
       "min      -1.128047  \n",
       "25%       0.084062  \n",
       "50%       0.308030  \n",
       "75%       0.581019  \n",
       "max       5.021393  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_outliers(model, X, y, sigma=3):\n",
    "    # 查找离群值\n",
    "    # 标准偏差预先已知的情况\n",
    "    # predict y values using model\n",
    "    try:\n",
    "        y_pred = pd.Series(model.predict(X), index=y.index)\n",
    "    # if predicting fails, try fitting the model first\n",
    "    except:\n",
    "        model.fit(X, y)\n",
    "        y_pred = pd.Series(model.predict(X), index=y.index)\n",
    "        \n",
    "    # calculate residuals between the model prediction and true y values\n",
    "    resid = y - y_pred\n",
    "    mean_resid = resid.mean()\n",
    "    std_resid = resid.std()\n",
    "    \n",
    "    # calculate z statistic, define outliers to be where |z|>sigma\n",
    "    z = (resid - mean_resid) / std_resid\n",
    "    outliers = z[abs(z) > sigma].index\n",
    "    \n",
    "    # print and plot the results\n",
    "    print('R2 =', model.score(X, y))\n",
    "    print('mse =', mean_squared_error(y, y_pred))\n",
    "    print('---------------------------------------')\n",
    "    \n",
    "    print('mean of residuals:', mean_resid)\n",
    "    print('std of residuals:', std_resid)\n",
    "    print('---------------------------------------')\n",
    "    \n",
    "    print(len(outliers), 'outliers: ')\n",
    "    print(outliers.tolist())\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    ax_131 = plt.subplot(1,3,1)\n",
    "    plt.plot(y, y_pred, '.')\n",
    "    plt.plot(y.loc[outliers], y_pred.loc[outliers], 'ro')\n",
    "    # loc[1]是DataFrame的行索引, loc[1,2]是行列索引\n",
    "    # o表示小圆圈，ro表示红色小圆圈\n",
    "    plt.legend(['Accepted', 'Outlier'])\n",
    "    plt.xlabel('y')\n",
    "    plt.ylabel('y_pred')\n",
    "    \n",
    "    ax_132 = plt.subplot(1,3,2)\n",
    "    plt.plot(y, y - y_pred, '.')\n",
    "    plt.plot(y.loc[outliers], y.loc[outliers] - y_pred.loc[outliers], 'ro')\n",
    "    plt.legend(['Accepted', 'Outlier'])\n",
    "    plt.xlabel('y')\n",
    "    plt.ylabel('y - y_pred')\n",
    "    \n",
    "    ax_133 = plt.subplot(1,3,3)\n",
    "    z.plot.hist(bins=50, ax=ax_133)\n",
    "    z.loc[outliers].plot.hist(color='r', bins=50, ax=ax_133)\n",
    "    plt.legend(['Accepted', 'Outlier'])\n",
    "    plt.xlabel('z')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 = 0.8792538079829968\n",
      "mse = 0.11686459392796991\n",
      "---------------------------------------\n",
      "mean of residuals: -4.182557655097543e-17\n",
      "std of residuals: 0.3419138393280762\n",
      "---------------------------------------\n",
      "33 outliers: \n",
      "[321, 344, 693, 776, 777, 884, 1296, 1310, 1458, 1523, 1704, 1825, 1874, 1905, 1934, 1972, 1979, 2002, 2160, 2264, 2279, 2620, 2645, 2667, 2668, 2669, 2696, 2697, 2767, 2769, 2807, 2842, 2863]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "outliers = find_outliers(Ridge(), X, y)\n",
    "\n",
    "X_outliers = X.loc[outliers]\n",
    "y_outliers = y.loc[outliers]\n",
    "X_t = X.drop(outliers)\n",
    "y_t = y.drop(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_EVALS = 1000\n",
    "N_FOLDS = 5\n",
    "XGB_MAX_LEAVES = 2**11\n",
    "XGB_MAX_DEPTH = 25 \n",
    "EVAL_METRIC_XGB_REG = 'rmse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "LGBM_MAX_LEAVES = 2**11\n",
    "LGBM_MAX_DEPTH = 25\n",
    "EVAL_METRIC_LGBM_REG = 'rmse'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quick_hyperopt(data, labels, package='lgbm', num_evals=NUM_EVALS, diagnostic=False):\n",
    "    \n",
    "    #==========\n",
    "    #LightGBM\n",
    "    #==========\n",
    "    \n",
    "    if package=='lgbm':\n",
    "        \n",
    "        print('Running {} rounds of LightGBM parameter optimisation:'.format(num_evals))\n",
    "        #clear space\n",
    "        gc.collect()\n",
    "        \n",
    "        integer_params = ['max_depth',\n",
    "                         'num_leaves',\n",
    "                          'max_bin',\n",
    "                         'min_data_in_leaf',\n",
    "                         'min_data_in_bin']\n",
    "        \n",
    "        def objective(space_params):\n",
    "            \n",
    "            #cast integer params from float to int\n",
    "            for param in integer_params:\n",
    "                space_params[param] = int(space_params[param])\n",
    "            \n",
    "            #extract nested conditional parameters\n",
    "            if space_params['boosting']['boosting'] == 'goss':\n",
    "                top_rate = space_params['boosting'].get('top_rate')\n",
    "                other_rate = space_params['boosting'].get('other_rate')\n",
    "                #0 <= top_rate + other_rate <= 1\n",
    "                top_rate = max(top_rate, 0)\n",
    "                top_rate = min(top_rate, 0.5)\n",
    "                other_rate = max(other_rate, 0)\n",
    "                other_rate = min(other_rate, 0.5)\n",
    "                space_params['top_rate'] = top_rate\n",
    "                space_params['other_rate'] = other_rate\n",
    "            \n",
    "            subsample = space_params['boosting'].get('subsample', 1.0)\n",
    "            space_params['boosting'] = space_params['boosting']['boosting']\n",
    "            space_params['subsample'] = subsample\n",
    "            \n",
    "            #for classification, set stratified=True and metrics=EVAL_METRIC_LGBM_CLASS\n",
    "            cv_results = lgb.cv(space_params, train, nfold = N_FOLDS, stratified=False,\n",
    "                                early_stopping_rounds=100, metrics=EVAL_METRIC_LGBM_REG, seed=42)\n",
    "            \n",
    "            best_loss = cv_results['rmse-mean'][-1] #'l2-mean' for rmse\n",
    "            #for classification, comment out the line above and uncomment the line below:\n",
    "            #best_loss = 1 - cv_results['auc-mean'][-1]\n",
    "            #if necessary, replace 'auc-mean' with '[your-preferred-metric]-mean'\n",
    "            return{'loss':best_loss, 'status': STATUS_OK }\n",
    "        \n",
    "        train = lgb.Dataset(data, labels)\n",
    "                \n",
    "        #integer and string parameters, used with hp.choice()\n",
    "        boosting_list = [{'boosting': 'gbdt',\n",
    "                          'subsample': hp.uniform('subsample', 0.5, 1)},\n",
    "                         {'boosting': 'goss',\n",
    "                          'subsample': 1.0,\n",
    "                         'top_rate': hp.uniform('top_rate', 0, 0.5),\n",
    "                         'other_rate': hp.uniform('other_rate', 0, 0.5)}] #if including 'dart', make sure to set 'n_estimators'\n",
    "        metric_list = ['RMSE'] \n",
    "        #for classification comment out the line above and uncomment the line below\n",
    "        #metric_list = ['auc'] #modify as required for other classification metrics\n",
    "        objective_list_reg = ['huber',  'fair']\n",
    "        objective_list_class = ['binary', 'cross_entropy']\n",
    "        #for classification set objective_list = objective_list_class\n",
    "        objective_list = objective_list_reg\n",
    "\n",
    "        space ={'boosting' : hp.choice('boosting', boosting_list),\n",
    "                'num_leaves' : hp.quniform('num_leaves', 2, LGBM_MAX_LEAVES, 1),\n",
    "                'max_depth': hp.quniform('max_depth', 2, LGBM_MAX_DEPTH, 1),\n",
    "                'max_bin': hp.quniform('max_bin', 32, 255, 1),\n",
    "                'min_data_in_leaf': hp.quniform('min_data_in_leaf', 1, 256, 1),\n",
    "                'min_data_in_bin': hp.quniform('min_data_in_bin', 1, 256, 1),\n",
    "                'min_gain_to_split' : hp.quniform('min_gain_to_split', 0.1, 5, 0.01),\n",
    "                'lambda_l1' : hp.uniform('lambda_l1', 0, 5),\n",
    "                'lambda_l2' : hp.uniform('lambda_l2', 0, 5),\n",
    "                'learning_rate' : hp.loguniform('learning_rate', np.log(0.005), np.log(0.2)),\n",
    "                'metric' : hp.choice('metric', metric_list),\n",
    "                'objective' : hp.choice('objective', objective_list),\n",
    "                'feature_fraction' : hp.quniform('feature_fraction', 0.5, 1, 0.01),\n",
    "                'bagging_fraction' : hp.quniform('bagging_fraction', 0.5, 1, 0.01)\n",
    "            }\n",
    "        \n",
    "        #optional: activate GPU for LightGBM\n",
    "        #follow compilation steps here:\n",
    "        #https://www.kaggle.com/vinhnguyen/gpu-acceleration-for-lightgbm/\n",
    "        #then uncomment lines below:\n",
    "        #space['device'] = 'gpu'\n",
    "        #space['gpu_platform_id'] = 0,\n",
    "        #space['gpu_device_id'] =  0\n",
    "\n",
    "        trials = Trials()\n",
    "        best = fmin(fn=objective,\n",
    "                    space=space,\n",
    "                    algo=tpe.suggest,\n",
    "                    max_evals=num_evals, \n",
    "                    trials=trials)\n",
    "                \n",
    "        #fmin() will return the index of values chosen from the lists/arrays in 'space'\n",
    "        #to obtain actual values, index values are used to subset the original lists/arrays\n",
    "        best['boosting'] = boosting_list[best['boosting']]['boosting']#nested dict, index twice\n",
    "        best['metric'] = metric_list[best['metric']]\n",
    "        best['objective'] = objective_list[best['objective']]\n",
    "                \n",
    "        #cast floats of integer params to int\n",
    "        for param in integer_params:\n",
    "            best[param] = int(best[param])\n",
    "        \n",
    "        print('{' + '\\n'.join('{}: {}'.format(k, v) for k, v in best.items()) + '}')\n",
    "        if diagnostic:\n",
    "            return(best, trials)\n",
    "        else:\n",
    "            return(best)\n",
    "    \n",
    "    #==========\n",
    "    #XGBoost\n",
    "    #==========\n",
    "    \n",
    "    if package=='xgb':\n",
    "        \n",
    "        print('Running {} rounds of XGBoost parameter optimisation:'.format(num_evals))\n",
    "        #clear space\n",
    "        gc.collect()\n",
    "        \n",
    "        integer_params = ['max_depth']\n",
    "        \n",
    "        def objective(space_params):\n",
    "            \n",
    "            for param in integer_params:\n",
    "                space_params[param] = int(space_params[param])\n",
    "                \n",
    "            #extract multiple nested tree_method conditional parameters\n",
    "            #libera te tutemet ex inferis\n",
    "            if space_params['tree_method']['tree_method'] == 'hist':\n",
    "                max_bin = space_params['tree_method'].get('max_bin')\n",
    "                space_params['max_bin'] = int(max_bin)\n",
    "                if space_params['tree_method']['grow_policy']['grow_policy']['grow_policy'] == 'depthwise':\n",
    "                    grow_policy = space_params['tree_method'].get('grow_policy').get('grow_policy').get('grow_policy')\n",
    "                    space_params['grow_policy'] = grow_policy\n",
    "                    space_params['tree_method'] = 'hist'\n",
    "                else:\n",
    "                    max_leaves = space_params['tree_method']['grow_policy']['grow_policy'].get('max_leaves')\n",
    "                    space_params['grow_policy'] = 'lossguide'\n",
    "                    space_params['max_leaves'] = int(max_leaves)\n",
    "                    space_params['tree_method'] = 'hist'\n",
    "            else:\n",
    "                space_params['tree_method'] = space_params['tree_method'].get('tree_method')\n",
    "                \n",
    "            #for classification replace EVAL_METRIC_XGB_REG with EVAL_METRIC_XGB_CLASS\n",
    "            cv_results = xgb.cv(space_params, train, nfold=N_FOLDS, metrics=[EVAL_METRIC_XGB_REG],\n",
    "                             early_stopping_rounds=100, stratified=False, seed=42)\n",
    "            \n",
    "            best_loss = cv_results['test-rmse-mean'].iloc[-1] #or 'test-rmse-mean' if using RMSE\n",
    "            #for classification, comment out the line above and uncomment the line below:\n",
    "            #best_loss = 1 - cv_results['test-auc-mean'].iloc[-1]\n",
    "            #if necessary, replace 'test-auc-mean' with 'test-[your-preferred-metric]-mean'\n",
    "            return{'loss':best_loss, 'status': STATUS_OK }\n",
    "        \n",
    "        train = xgb.DMatrix(data, labels)\n",
    "        \n",
    "        #integer and string parameters, used with hp.choice()\n",
    "        boosting_list = ['gbtree', 'gblinear'] #if including 'dart', make sure to set 'n_estimators'\n",
    "        metric_list = ['rmse'] \n",
    "        #for classification comment out the line above and uncomment the line below\n",
    "        #metric_list = ['auc']\n",
    "        #modify as required for other classification metrics classification\n",
    "        \n",
    "        tree_method = [{'tree_method' : 'exact'},\n",
    "               {'tree_method' : 'approx'},\n",
    "               {'tree_method' : 'hist',\n",
    "                'max_bin': hp.quniform('max_bin', 2**3, 2**7, 1),\n",
    "                'grow_policy' : {'grow_policy': {'grow_policy':'depthwise'},\n",
    "                                'grow_policy' : {'grow_policy':'lossguide',\n",
    "                                                  'max_leaves': hp.quniform('max_leaves', 32, XGB_MAX_LEAVES, 1)}}}]\n",
    "        \n",
    "        #if using GPU, replace 'exact' with 'gpu_exact' and 'hist' with\n",
    "        #'gpu_hist' in the nested dictionary above\n",
    "        \n",
    "        objective_list_reg = ['reg:linear']\n",
    "        objective_list_class = ['reg:logistic', 'binary:logistic']\n",
    "        #for classification change line below to 'objective_list = objective_list_class'\n",
    "        objective_list = objective_list_reg\n",
    "        \n",
    "        space ={'boosting' : hp.choice('boosting', boosting_list),\n",
    "                'tree_method' : hp.choice('tree_method', tree_method),\n",
    "                'max_depth': hp.quniform('max_depth', 2, XGB_MAX_DEPTH, 1),\n",
    "                'reg_alpha' : hp.uniform('reg_alpha', 0, 5),\n",
    "                'reg_lambda' : hp.uniform('reg_lambda', 0, 5),\n",
    "                'min_child_weight' : hp.uniform('min_child_weight', 0, 5),\n",
    "                'gamma' : hp.uniform('gamma', 0, 5),\n",
    "                'learning_rate' : hp.loguniform('learning_rate', np.log(0.005), np.log(0.2)),\n",
    "                'eval_metric' : hp.choice('eval_metric', metric_list),\n",
    "                'objective' : hp.choice('objective', objective_list),\n",
    "                'colsample_bytree' : hp.quniform('colsample_bytree', 0.1, 1, 0.01),\n",
    "                'colsample_bynode' : hp.quniform('colsample_bynode', 0.1, 1, 0.01),\n",
    "                'colsample_bylevel' : hp.quniform('colsample_bylevel', 0.1, 1, 0.01),\n",
    "                'subsample' : hp.quniform('subsample', 0.5, 1, 0.05),\n",
    "                'nthread' : 3\n",
    "            }\n",
    "        \n",
    "        trials = Trials()\n",
    "        best = fmin(fn=objective,\n",
    "                    space=space,\n",
    "                    algo=tpe.suggest,\n",
    "                    max_evals=num_evals, \n",
    "                    trials=trials)\n",
    "        \n",
    "        best['tree_method'] = tree_method[best['tree_method']]['tree_method']\n",
    "        best['boosting'] = boosting_list[best['boosting']]\n",
    "        best['eval_metric'] = metric_list[best['eval_metric']]\n",
    "        best['objective'] = objective_list[best['objective']]\n",
    "        \n",
    "        #cast floats of integer params to int\n",
    "        for param in integer_params:\n",
    "            best[param] = int(best[param])\n",
    "        if 'max_leaves' in best:\n",
    "            best['max_leaves'] = int(best['max_leaves'])\n",
    "        if 'max_bin' in best:\n",
    "            best['max_bin'] = int(best['max_bin'])\n",
    "        \n",
    "        print('{' + '\\n'.join('{}: {}'.format(k, v) for k, v in best.items()) + '}')\n",
    "        \n",
    "        if diagnostic:\n",
    "            return(best, trials)\n",
    "        else:\n",
    "            return(best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 2000 rounds of LightGBM parameter optimisation:\n",
      "100%|███████| 2000/2000 [24:58<00:00,  1.12s/it, best loss: 0.3415612222402958]\n",
      "{bagging_fraction: 0.56\n",
      "boosting: goss\n",
      "feature_fraction: 0.77\n",
      "lambda_l1: 0.18350148706536196\n",
      "lambda_l2: 3.260817180806637\n",
      "learning_rate: 0.09847962888249102\n",
      "max_bin: 125\n",
      "max_depth: 17\n",
      "metric: RMSE\n",
      "min_data_in_bin: 93\n",
      "min_data_in_leaf: 19\n",
      "min_gain_to_split: 0.11\n",
      "num_leaves: 1779\n",
      "objective: huber\n",
      "other_rate: 0.40918960559658013\n",
      "top_rate: 0.2678133759359054}\n"
     ]
    }
   ],
   "source": [
    "lgb_params = quick_hyperopt(X_t, y_t, package='lgbm', num_evals=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_params = {'bagging_fraction': 0.56,\n",
    "            'boosting': 'goss',\n",
    "            'feature_fraction': 0.77,\n",
    "            'lambda_l1': 0.18350148706536196,\n",
    "            'lambda_l2': 3.260817180806637,\n",
    "            'learning_rate': 0.09847962888249102,\n",
    "            'max_bin': 125,\n",
    "            'max_depth': 17,\n",
    "            'metric': 'RMSE',\n",
    "            'min_data_in_bin': 93,\n",
    "            'min_data_in_leaf': 19,\n",
    "            'min_gain_to_split': 0.11,\n",
    "            'num_leaves': 1779,\n",
    "            'objective': 'huber',\n",
    "            'other_rate': 0.40918960559658013,\n",
    "            'top_rate': 0.2678133759359054}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[133]\ttraining's rmse: 0.189415\tvalid_1's rmse: 0.343961\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[137]\ttraining's rmse: 0.187377\tvalid_1's rmse: 0.37711\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[72]\ttraining's rmse: 0.197902\tvalid_1's rmse: 0.333107\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[90]\ttraining's rmse: 0.190865\tvalid_1's rmse: 0.315871\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[85]\ttraining's rmse: 0.192759\tvalid_1's rmse: 0.359525\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[107]\ttraining's rmse: 0.190331\tvalid_1's rmse: 0.324749\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[112]\ttraining's rmse: 0.189933\tvalid_1's rmse: 0.352575\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[132]\ttraining's rmse: 0.187379\tvalid_1's rmse: 0.36812\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[98]\ttraining's rmse: 0.193575\tvalid_1's rmse: 0.332032\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[110]\ttraining's rmse: 0.191781\tvalid_1's rmse: 0.349089\n",
      "CV score: 0.11980723351606795, std: 0.012813405267478801\n"
     ]
    }
   ],
   "source": [
    "MSE = 0\n",
    "lgb_preds = np.zeros(len(test_data))\n",
    "oof_lgb = np.zeros(len(X_t))\n",
    "scores = []\n",
    "\n",
    "n_fold = 10\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=0)\n",
    "for fold, (train_idx, valid_idx) in enumerate(folds.split(y_t)):\n",
    "#     print(train_idx)\n",
    "    X_train, X_valid = X_t.iloc[train_idx], X_t.iloc[valid_idx]\n",
    "    y_train, y_valid = y_t.iloc[train_idx], y_t.iloc[valid_idx]\n",
    "    model = lgb.LGBMRegressor(**lgb_params, n_estimators=20000, n_jobs=3)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "              verbose=10000, early_stopping_rounds=1000)\n",
    "    val_preds = model.predict(X_valid, num_iteration=model.best_iteration_)\n",
    "    oof_lgb[valid_idx] = val_preds\n",
    "    MSE += mean_squared_error(y_valid, val_preds) / n_fold\n",
    "    lgb_preds += model.predict(test_data, num_iteration=model.best_iteration_) / n_fold\n",
    "    scores.append(mean_squared_error(y_valid, val_preds))\n",
    "    \n",
    "print('CV score: {}, std: {}'.format(MSE, np.std(scores)))\n",
    "# CV score: 0.11980723351606795, std: 0.012813405267478801"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 3000 rounds of XGBoost parameter optimisation:\n",
      "100%|█████████████████| 3000/3000 [42:25<00:00,  1.01it/s, best loss: 0.401007]\n",
      "{boosting: gbtree\n",
      "colsample_bylevel: 0.29\n",
      "colsample_bynode: 0.73\n",
      "colsample_bytree: 0.97\n",
      "eval_metric: rmse\n",
      "gamma: 0.41332145844972545\n",
      "learning_rate: 0.19964675210994973\n",
      "max_depth: 8\n",
      "min_child_weight: 3.3306534393618907\n",
      "objective: reg:linear\n",
      "reg_alpha: 0.3606307748824834\n",
      "reg_lambda: 0.30545212622045936\n",
      "subsample: 0.9500000000000001\n",
      "tree_method: approx}\n"
     ]
    }
   ],
   "source": [
    "xgb_params = quick_hyperopt(X_t, y_t, 'xgb', 3000, diagnostic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'boosting': 'gbtree',\n",
       " 'colsample_bylevel': 0.29,\n",
       " 'colsample_bynode': 0.73,\n",
       " 'colsample_bytree': 0.97,\n",
       " 'eval_metric': 'rmse',\n",
       " 'gamma': 0.41332145844972545,\n",
       " 'learning_rate': 0.19964675210994973,\n",
       " 'max_depth': 8,\n",
       " 'min_child_weight': 3.3306534393618907,\n",
       " 'objective': 'reg:linear',\n",
       " 'reg_alpha': 0.3606307748824834,\n",
       " 'reg_lambda': 0.30545212622045936,\n",
       " 'subsample': 0.9500000000000001,\n",
       " 'tree_method': 'approx'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_params = {'boosting': 'gbtree',\n",
    "             'colsample_bylevel': 0.29,\n",
    "             'colsample_bynode': 0.73,\n",
    "             'colsample_bytree': 0.97,\n",
    "             'eval_metric': 'rmse',\n",
    "             'gamma': 0.41332145844972545,\n",
    "             'learning_rate': 0.19964675210994973,\n",
    "             'max_depth': 8,\n",
    "             'min_child_weight': 3.3306534393618907,\n",
    "             'objective': 'reg:linear',\n",
    "             'reg_alpha': 0.3606307748824834,\n",
    "             'reg_lambda': 0.30545212622045936,\n",
    "             'subsample': 0.95,\n",
    "             'tree_method': 'approx'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:0.861423\tvalidation_1-rmse:0.858942\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 1000 rounds.\n",
      "Stopping. Best iteration:\n",
      "[328]\tvalidation_0-rmse:0.164295\tvalidation_1-rmse:0.361706\n",
      "\n",
      "[0]\tvalidation_0-rmse:0.856342\tvalidation_1-rmse:0.899168\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 1000 rounds.\n",
      "Stopping. Best iteration:\n",
      "[92]\tvalidation_0-rmse:0.168553\tvalidation_1-rmse:0.37237\n",
      "\n",
      "[0]\tvalidation_0-rmse:0.861382\tvalidation_1-rmse:0.866681\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 1000 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1862]\tvalidation_0-rmse:0.158074\tvalidation_1-rmse:0.347528\n",
      "\n",
      "[0]\tvalidation_0-rmse:0.857882\tvalidation_1-rmse:0.891747\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 1000 rounds.\n",
      "Stopping. Best iteration:\n",
      "[4354]\tvalidation_0-rmse:0.149357\tvalidation_1-rmse:0.342257\n",
      "\n",
      "[0]\tvalidation_0-rmse:0.855827\tvalidation_1-rmse:0.939525\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 1000 rounds.\n",
      "Stopping. Best iteration:\n",
      "[849]\tvalidation_0-rmse:0.157232\tvalidation_1-rmse:0.354802\n",
      "\n",
      "[0]\tvalidation_0-rmse:0.863957\tvalidation_1-rmse:0.839729\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 1000 rounds.\n",
      "Stopping. Best iteration:\n",
      "[4109]\tvalidation_0-rmse:0.151125\tvalidation_1-rmse:0.339043\n",
      "\n",
      "[0]\tvalidation_0-rmse:0.865114\tvalidation_1-rmse:0.833539\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 1000 rounds.\n",
      "Stopping. Best iteration:\n",
      "[1380]\tvalidation_0-rmse:0.153232\tvalidation_1-rmse:0.374914\n",
      "\n",
      "[0]\tvalidation_0-rmse:0.863435\tvalidation_1-rmse:0.8421\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 1000 rounds.\n",
      "Stopping. Best iteration:\n",
      "[192]\tvalidation_0-rmse:0.164446\tvalidation_1-rmse:0.384465\n",
      "\n",
      "[0]\tvalidation_0-rmse:0.860422\tvalidation_1-rmse:0.870498\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 1000 rounds.\n",
      "Stopping. Best iteration:\n",
      "[872]\tvalidation_0-rmse:0.159507\tvalidation_1-rmse:0.347191\n",
      "\n",
      "[0]\tvalidation_0-rmse:0.853518\tvalidation_1-rmse:0.925887\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 1000 rounds.\n",
      "Stopping. Best iteration:\n",
      "[268]\tvalidation_0-rmse:0.164857\tvalidation_1-rmse:0.369525\n",
      "\n",
      "CV score: 0.1293704436757454, std: 0.010615129325500969\n"
     ]
    }
   ],
   "source": [
    "MSE = 0\n",
    "xgb_preds = np.zeros(len(test_data))\n",
    "oof_xgb = np.zeros(len(X_t))\n",
    "scores = []\n",
    "\n",
    "n_fold = 10\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=0)\n",
    "for train_idx, valid_idx in folds.split(y_t):\n",
    "#     print(train_idx)\n",
    "    X_train, X_valid = X_t.iloc[train_idx], X_t.iloc[valid_idx]\n",
    "    y_train, y_valid = y_t.iloc[train_idx], y_t.iloc[valid_idx]\n",
    "    model = xgb.XGBRegressor(**xgb_params, n_estimators=20000, early_stopping_rounds=1000, n_jobs=3)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "              verbose=10000, early_stopping_rounds=1000)\n",
    "    val_preds = model.predict(X_valid)\n",
    "    oof_xgb[valid_idx] = val_preds\n",
    "    MSE += mean_squared_error(y_valid, val_preds) / n_fold\n",
    "    xgb_preds += model.predict(test_data) / n_fold\n",
    "    scores.append(mean_squared_error(y_valid, val_preds))\n",
    "    \n",
    "print('CV score: {}, std: {}'.format(MSE, np.std(scores)))\n",
    "# CV score: 0.1293704436757454, std: 0.010615129325500969"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████| 2000/2000 [53:25<00:00,  1.60s/it, best loss: 0.17184846715236426]\n"
     ]
    }
   ],
   "source": [
    "def objective_rf(params):\n",
    "    \n",
    "    model = RandomForestRegressor(criterion='mse')\n",
    "    scores = cross_val_score(model, X_t, y_t, cv=5, scoring='neg_mean_squared_error')\n",
    "    MSE = -scores.mean()\n",
    "    return {'loss': MSE, 'status': STATUS_OK}\n",
    "\n",
    "space = {'n_estimators': hp.choice('n_estimators', range(20, 2000)),\n",
    "         'max_depth': hp.choice('max_depth', range(3, 30)), \n",
    "         'min_samples_split': hp.choice('min_samples_split', range(2, 20)), \n",
    "         'min_samples_leaf': hp.choice('min_samples_leaf', range(2, 20)),\n",
    "         'max_features': hp.choice('max_features', range(3, 25)),\n",
    "         'n_jobs': 3}\n",
    "\n",
    "rf_params = fmin(fn=objective_rf,\n",
    "                 space=space,\n",
    "                 algo=tpe.suggest,\n",
    "                 max_evals=2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 8,\n",
       " 'max_features': 15,\n",
       " 'min_samples_leaf': 9,\n",
       " 'min_samples_split': 5,\n",
       " 'n_estimators': 1805}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_params = {'max_depth': 8,\n",
    "             'max_features': 15,\n",
    "             'min_samples_leaf': 9,\n",
    "             'min_samples_split': 5,\n",
    "             'n_estimators': 1805}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.15141623109709965\n"
     ]
    }
   ],
   "source": [
    "MSE = 0\n",
    "rf_preds = np.zeros(len(test_data))\n",
    "oof_rf = np.zeros(len(X_t))\n",
    "\n",
    "n_fold = 10\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=0)\n",
    "for train_idx, valid_idx in folds.split(y_t):\n",
    "#     print(train_idx)\n",
    "    X_train, X_valid = X_t.iloc[train_idx], X_t.iloc[valid_idx]\n",
    "    y_train, y_valid = y_t.iloc[train_idx], y_t.iloc[valid_idx]\n",
    "    rf = RandomForestRegressor(**rf_params)\n",
    "    rf.fit(X_train, y_train)\n",
    "    val_preds = rf.predict(X_valid)\n",
    "    oof_rf[valid_idx] = val_preds\n",
    "    MSE += mean_squared_error(y_valid, val_preds) / n_fold\n",
    "    rf_preds += rf.predict(test_data) / n_fold\n",
    "    \n",
    "print('CV score: {}'.format(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2853,)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oof_rf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1925,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_preds.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████| 1000/1000 [24:10<00:00,  1.37s/it, best loss: 0.09280198215777763]\n"
     ]
    }
   ],
   "source": [
    "def objective_kr(params):\n",
    "    \n",
    "    MSE = 0\n",
    "    n_fold = 5\n",
    "    folds = KFold(n_splits=n_fold, shuffle=True, random_state=0)\n",
    "    for train_idx, valid_idx in folds.split(y_t):\n",
    "    #     print(train_idx)\n",
    "        X_train, X_valid = X_t.iloc[train_idx], X_t.iloc[valid_idx]\n",
    "        y_train, y_valid = y_t.iloc[train_idx], y_t.iloc[valid_idx]\n",
    "        model = KernelRidge(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        val_preds = model.predict(X_valid)\n",
    "        MSE += mean_squared_error(y_valid, val_preds) / n_fold\n",
    "\n",
    "    return {'loss': MSE, 'status': STATUS_OK}\n",
    "\n",
    "space = {'alpha': hp.uniform('alpha', 0.01, 10),\n",
    "         'kernel': hp.choice('kernel', ['linear', 'rbf', 'poly']),\n",
    "         'gamma': hp.uniform('gamma', 0.01, 1),\n",
    "         'degree': hp.choice('degree', range(1, 4)),\n",
    "         'coef0': hp.uniform('coef0', 0, 10)}\n",
    "\n",
    "kr_params = fmin(fn=objective_kr,\n",
    "                 space=space,\n",
    "                 algo=tpe.suggest,\n",
    "                 max_evals=1000)\n",
    "\n",
    "kr_params['kernel'] = ['linear', 'rbf', 'poly'][kr_params['kernel']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 3.450600459134799,\n",
       " 'coef0': 7.689915318550242,\n",
       " 'degree': 1,\n",
       " 'gamma': 0.02394333869271543,\n",
       " 'kernel': 'poly'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kr_params = {'alpha': 3.450600459134799,\n",
    "             'coef0': 7.689915318550242,\n",
    "             'degree': 1,\n",
    "             'gamma': 0.02394333869271543,\n",
    "             'kernel': 'poly'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.10262846478585629\n"
     ]
    }
   ],
   "source": [
    "MSE = 0\n",
    "kr_preds = np.zeros(len(test_data))\n",
    "oof_kr = np.zeros(len(X_t))\n",
    "\n",
    "n_fold = 10\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=0)\n",
    "for train_idx, valid_idx in folds.split(y_t):\n",
    "    X_train, X_valid = X_t.iloc[train_idx], X_t.iloc[valid_idx]\n",
    "    y_train, y_valid = y_t.iloc[train_idx], y_t.iloc[valid_idx]\n",
    "    kr = KernelRidge(**kr_params)\n",
    "    kr.fit(X_train, y_train)\n",
    "    val_preds = kr.predict(X_valid)\n",
    "    oof_kr[valid_idx] = val_preds\n",
    "    MSE += mean_squared_error(y_valid, val_preds) / n_fold\n",
    "    kr_preds += kr.predict(test_data) / n_fold\n",
    "    \n",
    "print('CV score: {}'.format(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|███| 1000/1000 [16:49:43<00:00, 78.70s/it, best loss: 0.12577162101709474] \n"
     ]
    }
   ],
   "source": [
    "def objective_gbm(params):\n",
    "    \n",
    "    MSE = 0\n",
    "    n_fold = 5\n",
    "    folds = KFold(n_splits=n_fold, shuffle=True, random_state=0)\n",
    "    \n",
    "    model = GradientBoostingRegressor(**params, n_iter_no_change=200)\n",
    "    scores = cross_val_score(model, X_t, y_t, cv=3, scoring='neg_mean_squared_error')\n",
    "    MSE = -scores.mean()\n",
    "    return {'loss': MSE, 'status': STATUS_OK}\n",
    "\n",
    "\n",
    "\n",
    "space = {'loss': hp.choice('loss', ['ls', 'lad', 'huber', 'quantile']),\n",
    "         'learning_rate': hp.loguniform('learning_rate', np.log(0.005), np.log(0.2)),\n",
    "         'n_estimators': hp.choice('n_estimators', range(5000, 8000)),\n",
    "         'max_depth': hp.choice('max_depth', range(3, 30)),\n",
    "         'min_samples_leaf': hp.choice('min_samples_leaf', range(2, 30)),\n",
    "         'min_samples_split': hp.choice('min_samples_split', range(2, 300)),\n",
    "         'max_features': hp.choice('max_features', range(1, 20)),\n",
    "         'alpha': hp.uniform('alpha', 0, 1)}\n",
    "\n",
    "gbm_params = fmin(fn=objective_gbm,\n",
    "                  space=space,\n",
    "                  algo=tpe.suggest,\n",
    "                  max_evals=1000)\n",
    "\n",
    "gbm_params['loss'] = ['ls', 'lad', 'huber', 'quantile'][gbm_params['loss']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.9360679087587133,\n",
       " 'learning_rate': 0.005426460373788982,\n",
       " 'loss': 'huber',\n",
       " 'max_depth': 1,\n",
       " 'max_features': 9,\n",
       " 'min_samples_leaf': 14,\n",
       " 'min_samples_split': 290,\n",
       " 'n_estimators': 1727}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gbm_params = {'alpha': 0.9360679087587133,\n",
    "             'learning_rate': 0.005426460373788982,\n",
    "             'loss': 'huber',\n",
    "             'max_depth': 1,\n",
    "             'max_features': 9,\n",
    "             'min_samples_leaf': 14,\n",
    "             'min_samples_split': 290,\n",
    "             'n_estimators': 1727}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.18252688313861534\n"
     ]
    }
   ],
   "source": [
    "MSE = 0\n",
    "gbm_preds = np.zeros(len(test_data))\n",
    "oof_gbm = np.zeros(len(X_t))\n",
    "\n",
    "n_fold = 10\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=0)\n",
    "for train_idx, valid_idx in folds.split(y_t):\n",
    "    X_train, X_valid = X_t.iloc[train_idx], X_t.iloc[valid_idx]\n",
    "    y_train, y_valid = y_t.iloc[train_idx], y_t.iloc[valid_idx]\n",
    "    gbm = GradientBoostingRegressor(**gbm_params)\n",
    "    gbm.fit(X_train, y_train)\n",
    "    val_preds = gbm.predict(X_valid)\n",
    "    oof_gbm[valid_idx] = val_preds\n",
    "    MSE += mean_squared_error(y_valid, val_preds) / n_fold\n",
    "    gbm_preds += gbm.predict(test_data) / n_fold\n",
    "    \n",
    "print('CV score: {}'.format(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%| | 1/1000 [01:03<17:38:55, 63.60s/it, best loss: 0.10785151866901369]"
     ]
    }
   ],
   "source": [
    "def objective_svr(params):\n",
    "    model = NuSVR(**params)\n",
    "    scores = cross_val_score(model, X_t, y_t, cv=5, scoring='neg_mean_squared_error')\n",
    "    MSE = -scores.mean()\n",
    "    return {'loss': MSE, 'status': STATUS_OK}\n",
    "\n",
    "space = {'C': hp.uniform('C', 0.1, 10.0),\n",
    "         'nu': hp.uniform('nu', 0, 1.0),\n",
    "         'kernel': hp.choice('kernel', ['linear', 'poly', 'rbf']),\n",
    "         'degree': hp.choice('degree', [1, 2, 3]),\n",
    "         'gamma': hp.uniform('gamma', 0.01, 10),\n",
    "         'coef0': hp.uniform('coef0', 0, 10)}\n",
    "\n",
    "svr_params = fmin(fn=objective_svr,\n",
    "                  space=space,\n",
    "                  algo=tpe.suggest,\n",
    "                  max_evals=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_params1 = {'gamma': 'scale', 'nu': 0.9, 'C': 10.0, 'tol': 0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.12962938077053374\n"
     ]
    }
   ],
   "source": [
    "MSE = 0\n",
    "svr_preds1 = np.zeros(len(test_data))\n",
    "oof_svr1 = np.zeros(len(X_t))\n",
    "\n",
    "n_fold = 10\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=0)\n",
    "for train_idx, valid_idx in folds.split(y_t):\n",
    "    X_train, X_valid = X_t.iloc[train_idx], X_t.iloc[valid_idx]\n",
    "    y_train, y_valid = y_t.iloc[train_idx], y_t.iloc[valid_idx]\n",
    "    svr1 = NuSVR(**svr_params1)\n",
    "    svr1.fit(X_train, y_train)\n",
    "    val_preds = svr1.predict(X_valid)\n",
    "    oof_svr1[valid_idx] = val_preds\n",
    "    MSE += mean_squared_error(y_valid, val_preds) / n_fold\n",
    "    svr_preds1 += svr1.predict(test_data) / n_fold\n",
    "    \n",
    "print('CV score: {}'.format(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "svr_params2 = {'gamma': 'scale', 'nu': 0.7, 'C': 1.0, 'tol': 0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.11459351154088783\n"
     ]
    }
   ],
   "source": [
    "MSE = 0\n",
    "svr_preds2 = np.zeros(len(test_data))\n",
    "oof_svr2 = np.zeros(len(X_t))\n",
    "\n",
    "n_fold = 10\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=0)\n",
    "for train_idx, valid_idx in folds.split(y_t):\n",
    "    X_train, X_valid = X_t.iloc[train_idx], X_t.iloc[valid_idx]\n",
    "    y_train, y_valid = y_t.iloc[train_idx], y_t.iloc[valid_idx]\n",
    "    svr2 = NuSVR(**svr_params2)\n",
    "    svr2.fit(X_train, y_train)\n",
    "    val_preds = svr2.predict(X_valid)\n",
    "    oof_svr2[valid_idx] = val_preds\n",
    "    MSE += mean_squared_error(y_valid, val_preds) / n_fold\n",
    "    svr_preds2 += svr2.predict(test_data) / n_fold\n",
    "    \n",
    "print('CV score: {}'.format(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(criterion='mse', max_depth=13, min_samples_split=23, min_samples_leaf=6),  \n",
    "                        loss='square', learning_rate=1.0, n_estimators=700, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.12489809336415758\n"
     ]
    }
   ],
   "source": [
    "MSE = 0\n",
    "ada_preds = np.zeros(len(test_data))\n",
    "oof_ada = np.zeros(len(X_t))\n",
    "\n",
    "n_fold = 10\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=0)\n",
    "for train_idx, valid_idx in folds.split(y_t):\n",
    "    X_train, X_valid = X_t.iloc[train_idx], X_t.iloc[valid_idx]\n",
    "    y_train, y_valid = y_t.iloc[train_idx], y_t.iloc[valid_idx]\n",
    "    ada.fit(X_train, y_train)\n",
    "    val_preds = ada.predict(X_valid)\n",
    "    oof_ada[valid_idx] = val_preds\n",
    "    MSE += mean_squared_error(y_valid, val_preds) / n_fold\n",
    "    ada_preds += ada.predict(test_data) / n_fold\n",
    "    \n",
    "print('CV score: {}'.format(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "stack_list = [oof_lgb, oof_xgb, oof_rf, oof_ada]\n",
    "pred_list = [lgb_preds, xgb_preds, rf_preds, ada_preds]\n",
    "columns = ['lgb', 'xgb', 'rf', 'ada']\n",
    "train_stack = pd.DataFrame(np.vstack(stack_list).transpose(), columns=columns)\n",
    "test_stack = pd.DataFrame(np.vstack(pred_list).transpose(), columns=columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lgb</th>\n",
       "      <th>xgb</th>\n",
       "      <th>rf</th>\n",
       "      <th>ada</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.133082</td>\n",
       "      <td>0.429748</td>\n",
       "      <td>-0.198855</td>\n",
       "      <td>0.150182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.303197</td>\n",
       "      <td>0.232465</td>\n",
       "      <td>0.260532</td>\n",
       "      <td>0.298267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.410074</td>\n",
       "      <td>0.181126</td>\n",
       "      <td>0.369354</td>\n",
       "      <td>0.515667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.312226</td>\n",
       "      <td>0.403242</td>\n",
       "      <td>0.241236</td>\n",
       "      <td>0.357833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.298027</td>\n",
       "      <td>0.179713</td>\n",
       "      <td>0.245468</td>\n",
       "      <td>0.262947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.469155</td>\n",
       "      <td>0.465400</td>\n",
       "      <td>0.398665</td>\n",
       "      <td>0.355000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.580365</td>\n",
       "      <td>0.520977</td>\n",
       "      <td>0.477321</td>\n",
       "      <td>0.429909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.662979</td>\n",
       "      <td>0.595233</td>\n",
       "      <td>0.519993</td>\n",
       "      <td>0.546238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.761885</td>\n",
       "      <td>0.740916</td>\n",
       "      <td>0.604001</td>\n",
       "      <td>0.657333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1.099321</td>\n",
       "      <td>1.031395</td>\n",
       "      <td>0.821436</td>\n",
       "      <td>1.017150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1.145985</td>\n",
       "      <td>1.001495</td>\n",
       "      <td>0.886098</td>\n",
       "      <td>1.040579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.141512</td>\n",
       "      <td>1.193186</td>\n",
       "      <td>0.925383</td>\n",
       "      <td>0.988000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-1.142964</td>\n",
       "      <td>-0.960878</td>\n",
       "      <td>-1.151185</td>\n",
       "      <td>-1.155733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>-1.818472</td>\n",
       "      <td>-1.753587</td>\n",
       "      <td>-1.479591</td>\n",
       "      <td>-1.386650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.481424</td>\n",
       "      <td>-0.621829</td>\n",
       "      <td>-0.959803</td>\n",
       "      <td>-0.710250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-1.710190</td>\n",
       "      <td>-1.715983</td>\n",
       "      <td>-1.498013</td>\n",
       "      <td>-1.453947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>-0.617471</td>\n",
       "      <td>-0.718585</td>\n",
       "      <td>-0.965290</td>\n",
       "      <td>-0.967000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>-1.831032</td>\n",
       "      <td>-1.425328</td>\n",
       "      <td>-1.524461</td>\n",
       "      <td>-1.550143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>-0.735703</td>\n",
       "      <td>-0.340708</td>\n",
       "      <td>-0.762192</td>\n",
       "      <td>-0.553857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.060412</td>\n",
       "      <td>0.120766</td>\n",
       "      <td>0.024353</td>\n",
       "      <td>0.118167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.264170</td>\n",
       "      <td>0.246947</td>\n",
       "      <td>0.165045</td>\n",
       "      <td>0.243750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.165881</td>\n",
       "      <td>0.002377</td>\n",
       "      <td>-0.018427</td>\n",
       "      <td>0.054067</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.428184</td>\n",
       "      <td>0.505050</td>\n",
       "      <td>0.260006</td>\n",
       "      <td>0.346000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>-0.001382</td>\n",
       "      <td>-0.018073</td>\n",
       "      <td>-0.129714</td>\n",
       "      <td>-0.040824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>-2.311416</td>\n",
       "      <td>-2.340839</td>\n",
       "      <td>-2.179034</td>\n",
       "      <td>-2.315818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>-2.217322</td>\n",
       "      <td>-2.525402</td>\n",
       "      <td>-2.029054</td>\n",
       "      <td>-2.220529</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.103215</td>\n",
       "      <td>0.305602</td>\n",
       "      <td>0.078169</td>\n",
       "      <td>0.110467</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.649661</td>\n",
       "      <td>0.647547</td>\n",
       "      <td>0.644954</td>\n",
       "      <td>0.778680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1.084884</td>\n",
       "      <td>0.940755</td>\n",
       "      <td>0.848394</td>\n",
       "      <td>1.037000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.457520</td>\n",
       "      <td>0.424830</td>\n",
       "      <td>0.587397</td>\n",
       "      <td>0.544133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2825</th>\n",
       "      <td>0.137929</td>\n",
       "      <td>0.101493</td>\n",
       "      <td>-0.042104</td>\n",
       "      <td>0.002818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2826</th>\n",
       "      <td>-0.231815</td>\n",
       "      <td>-0.183157</td>\n",
       "      <td>-0.449256</td>\n",
       "      <td>-0.313667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2827</th>\n",
       "      <td>-0.255906</td>\n",
       "      <td>-0.203365</td>\n",
       "      <td>-0.103198</td>\n",
       "      <td>-0.065273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2828</th>\n",
       "      <td>0.678162</td>\n",
       "      <td>0.677213</td>\n",
       "      <td>0.471675</td>\n",
       "      <td>0.532818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2829</th>\n",
       "      <td>0.695654</td>\n",
       "      <td>0.603765</td>\n",
       "      <td>0.505739</td>\n",
       "      <td>0.527211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2830</th>\n",
       "      <td>-0.310812</td>\n",
       "      <td>-0.526840</td>\n",
       "      <td>-0.398864</td>\n",
       "      <td>-0.427737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2831</th>\n",
       "      <td>0.226944</td>\n",
       "      <td>-0.061743</td>\n",
       "      <td>-0.148400</td>\n",
       "      <td>0.004190</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2832</th>\n",
       "      <td>-0.948551</td>\n",
       "      <td>-0.945602</td>\n",
       "      <td>-0.769844</td>\n",
       "      <td>-0.844600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2833</th>\n",
       "      <td>-0.726872</td>\n",
       "      <td>-0.621448</td>\n",
       "      <td>-0.635741</td>\n",
       "      <td>-0.633450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2834</th>\n",
       "      <td>0.027149</td>\n",
       "      <td>-0.108805</td>\n",
       "      <td>-0.142405</td>\n",
       "      <td>0.009056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2835</th>\n",
       "      <td>0.022172</td>\n",
       "      <td>0.059735</td>\n",
       "      <td>-0.108298</td>\n",
       "      <td>-0.125667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2836</th>\n",
       "      <td>0.367522</td>\n",
       "      <td>0.286854</td>\n",
       "      <td>0.287392</td>\n",
       "      <td>0.284471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2837</th>\n",
       "      <td>0.177765</td>\n",
       "      <td>0.154668</td>\n",
       "      <td>0.321448</td>\n",
       "      <td>0.291125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2838</th>\n",
       "      <td>0.366440</td>\n",
       "      <td>0.301077</td>\n",
       "      <td>0.417718</td>\n",
       "      <td>0.373714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2839</th>\n",
       "      <td>0.469929</td>\n",
       "      <td>0.566361</td>\n",
       "      <td>0.350610</td>\n",
       "      <td>0.510176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2840</th>\n",
       "      <td>-0.087972</td>\n",
       "      <td>0.123957</td>\n",
       "      <td>-0.172307</td>\n",
       "      <td>-0.041238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2841</th>\n",
       "      <td>0.261578</td>\n",
       "      <td>0.481643</td>\n",
       "      <td>0.299364</td>\n",
       "      <td>0.366111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2842</th>\n",
       "      <td>-0.148696</td>\n",
       "      <td>-0.185062</td>\n",
       "      <td>-0.281476</td>\n",
       "      <td>-0.200571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2843</th>\n",
       "      <td>-0.186197</td>\n",
       "      <td>-0.194504</td>\n",
       "      <td>-0.221451</td>\n",
       "      <td>-0.157056</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2844</th>\n",
       "      <td>0.290191</td>\n",
       "      <td>0.263709</td>\n",
       "      <td>0.039942</td>\n",
       "      <td>0.125667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2845</th>\n",
       "      <td>0.598502</td>\n",
       "      <td>0.497541</td>\n",
       "      <td>0.278737</td>\n",
       "      <td>0.466250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2846</th>\n",
       "      <td>0.585598</td>\n",
       "      <td>0.582735</td>\n",
       "      <td>0.308754</td>\n",
       "      <td>0.445600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2847</th>\n",
       "      <td>0.676326</td>\n",
       "      <td>0.664797</td>\n",
       "      <td>0.436706</td>\n",
       "      <td>0.639059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2848</th>\n",
       "      <td>0.839085</td>\n",
       "      <td>0.846469</td>\n",
       "      <td>0.616870</td>\n",
       "      <td>0.749938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2849</th>\n",
       "      <td>0.759764</td>\n",
       "      <td>0.721308</td>\n",
       "      <td>0.566743</td>\n",
       "      <td>0.626941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2850</th>\n",
       "      <td>0.166831</td>\n",
       "      <td>0.229790</td>\n",
       "      <td>0.148409</td>\n",
       "      <td>0.255235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2851</th>\n",
       "      <td>0.699658</td>\n",
       "      <td>0.770492</td>\n",
       "      <td>0.443662</td>\n",
       "      <td>0.607550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2852</th>\n",
       "      <td>-0.358700</td>\n",
       "      <td>-0.285934</td>\n",
       "      <td>-0.208639</td>\n",
       "      <td>-0.359136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2853</th>\n",
       "      <td>-0.014195</td>\n",
       "      <td>0.022334</td>\n",
       "      <td>-0.131302</td>\n",
       "      <td>-0.188053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2854</th>\n",
       "      <td>0.383647</td>\n",
       "      <td>0.459300</td>\n",
       "      <td>0.019209</td>\n",
       "      <td>0.241500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2855 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           lgb       xgb        rf       ada\n",
       "0     0.133082  0.429748 -0.198855  0.150182\n",
       "1     0.303197  0.232465  0.260532  0.298267\n",
       "2     0.410074  0.181126  0.369354  0.515667\n",
       "3     0.312226  0.403242  0.241236  0.357833\n",
       "4     0.298027  0.179713  0.245468  0.262947\n",
       "5     0.469155  0.465400  0.398665  0.355000\n",
       "6     0.580365  0.520977  0.477321  0.429909\n",
       "7     0.662979  0.595233  0.519993  0.546238\n",
       "8     0.761885  0.740916  0.604001  0.657333\n",
       "9     1.099321  1.031395  0.821436  1.017150\n",
       "10    1.145985  1.001495  0.886098  1.040579\n",
       "11    1.141512  1.193186  0.925383  0.988000\n",
       "12   -1.142964 -0.960878 -1.151185 -1.155733\n",
       "13   -1.818472 -1.753587 -1.479591 -1.386650\n",
       "14   -0.481424 -0.621829 -0.959803 -0.710250\n",
       "15   -1.710190 -1.715983 -1.498013 -1.453947\n",
       "16   -0.617471 -0.718585 -0.965290 -0.967000\n",
       "17   -1.831032 -1.425328 -1.524461 -1.550143\n",
       "18   -0.735703 -0.340708 -0.762192 -0.553857\n",
       "19    0.060412  0.120766  0.024353  0.118167\n",
       "20    0.264170  0.246947  0.165045  0.243750\n",
       "21    0.165881  0.002377 -0.018427  0.054067\n",
       "22    0.428184  0.505050  0.260006  0.346000\n",
       "23   -0.001382 -0.018073 -0.129714 -0.040824\n",
       "24   -2.311416 -2.340839 -2.179034 -2.315818\n",
       "25   -2.217322 -2.525402 -2.029054 -2.220529\n",
       "26    0.103215  0.305602  0.078169  0.110467\n",
       "27    0.649661  0.647547  0.644954  0.778680\n",
       "28    1.084884  0.940755  0.848394  1.037000\n",
       "29    0.457520  0.424830  0.587397  0.544133\n",
       "...        ...       ...       ...       ...\n",
       "2825  0.137929  0.101493 -0.042104  0.002818\n",
       "2826 -0.231815 -0.183157 -0.449256 -0.313667\n",
       "2827 -0.255906 -0.203365 -0.103198 -0.065273\n",
       "2828  0.678162  0.677213  0.471675  0.532818\n",
       "2829  0.695654  0.603765  0.505739  0.527211\n",
       "2830 -0.310812 -0.526840 -0.398864 -0.427737\n",
       "2831  0.226944 -0.061743 -0.148400  0.004190\n",
       "2832 -0.948551 -0.945602 -0.769844 -0.844600\n",
       "2833 -0.726872 -0.621448 -0.635741 -0.633450\n",
       "2834  0.027149 -0.108805 -0.142405  0.009056\n",
       "2835  0.022172  0.059735 -0.108298 -0.125667\n",
       "2836  0.367522  0.286854  0.287392  0.284471\n",
       "2837  0.177765  0.154668  0.321448  0.291125\n",
       "2838  0.366440  0.301077  0.417718  0.373714\n",
       "2839  0.469929  0.566361  0.350610  0.510176\n",
       "2840 -0.087972  0.123957 -0.172307 -0.041238\n",
       "2841  0.261578  0.481643  0.299364  0.366111\n",
       "2842 -0.148696 -0.185062 -0.281476 -0.200571\n",
       "2843 -0.186197 -0.194504 -0.221451 -0.157056\n",
       "2844  0.290191  0.263709  0.039942  0.125667\n",
       "2845  0.598502  0.497541  0.278737  0.466250\n",
       "2846  0.585598  0.582735  0.308754  0.445600\n",
       "2847  0.676326  0.664797  0.436706  0.639059\n",
       "2848  0.839085  0.846469  0.616870  0.749938\n",
       "2849  0.759764  0.721308  0.566743  0.626941\n",
       "2850  0.166831  0.229790  0.148409  0.255235\n",
       "2851  0.699658  0.770492  0.443662  0.607550\n",
       "2852 -0.358700 -0.285934 -0.208639 -0.359136\n",
       "2853 -0.014195  0.022334 -0.131302 -0.188053\n",
       "2854  0.383647  0.459300  0.019209  0.241500\n",
       "\n",
       "[2855 rows x 4 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lgb</th>\n",
       "      <th>xgb</th>\n",
       "      <th>rf</th>\n",
       "      <th>ada</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lgb</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.982823</td>\n",
       "      <td>0.980628</td>\n",
       "      <td>0.986383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>xgb</th>\n",
       "      <td>0.982823</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.978226</td>\n",
       "      <td>0.984850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rf</th>\n",
       "      <td>0.980628</td>\n",
       "      <td>0.978226</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.989553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ada</th>\n",
       "      <td>0.986383</td>\n",
       "      <td>0.984850</td>\n",
       "      <td>0.989553</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          lgb       xgb        rf       ada\n",
       "lgb  1.000000  0.982823  0.980628  0.986383\n",
       "xgb  0.982823  1.000000  0.978226  0.984850\n",
       "rf   0.980628  0.978226  1.000000  0.989553\n",
       "ada  0.986383  0.984850  0.989553  1.000000"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_stack.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|██████| 1000/1000 [20:00<00:00,  1.22s/it, best loss: 0.11692760031497108]\n"
     ]
    }
   ],
   "source": [
    "def objective_kr2(params):\n",
    "    \n",
    "    MSE = 0\n",
    "    n_fold = 5\n",
    "    folds = KFold(n_splits=n_fold, shuffle=True, random_state=0)\n",
    "    for train_idx, valid_idx in folds.split(y_t):\n",
    "    #     print(train_idx)\n",
    "        X_train, X_valid = train_stack.iloc[train_idx], train_stack.iloc[valid_idx]\n",
    "        y_train, y_valid = y_t.iloc[train_idx], y_t.iloc[valid_idx]\n",
    "        model = KernelRidge(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        val_preds = model.predict(X_valid)\n",
    "        MSE += mean_squared_error(y_valid, val_preds) / n_fold\n",
    "\n",
    "    return {'loss': MSE, 'status': STATUS_OK}\n",
    "\n",
    "space = {'alpha': hp.uniform('alpha', 0.01, 10),\n",
    "         'kernel': hp.choice('kernel', ['linear']),\n",
    "         'gamma': hp.uniform('gamma', 0.01, 1)}\n",
    "\n",
    "kr_params2 = fmin(fn=objective_kr2,\n",
    "                  space=space,\n",
    "                  algo=tpe.suggest,\n",
    "                  max_evals=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1.2377655531308462, 'gamma': 0.45466358839670284, 'kernel': 'linear'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kr_params2 = {'alpha': 1.2377655531308462, 'gamma': 0.45466358839670284, 'kernel': 'linear'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV score: 0.11677229353555461\n"
     ]
    }
   ],
   "source": [
    "MSE = 0\n",
    "kr_preds2 = np.zeros(len(test_stack))\n",
    "\n",
    "n_fold = 10\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=0)\n",
    "for train_idx, valid_idx in folds.split(y_t):\n",
    "    X_train, X_valid = train_stack.iloc[train_idx], train_stack.iloc[valid_idx]\n",
    "    y_train, y_valid = y_t.iloc[train_idx], y_t.iloc[valid_idx]\n",
    "    kr2 = KernelRidge(**kr_params2)\n",
    "    kr2.fit(X_train, y_train)\n",
    "    val_preds = kr2.predict(X_valid)\n",
    "    MSE += mean_squared_error(y_valid, val_preds) / n_fold\n",
    "    kr_preds2 += kr2.predict(test_stack) / n_fold\n",
    "    \n",
    "print('CV score: {}'.format(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.44636873,  0.11846287,  0.08916632, ..., -1.93910992,\n",
       "       -1.96567283, -1.70841064])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kr_preds2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(kr_preds2).to_csv('stack_lgb_xgb_rf.txt', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params2 = quick_hyperopt(train_stack, y_t, 'xgb', 3000, diagnostic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-rmse:0.845798\tvalidation_1-rmse:0.858913\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 1000 rounds.\n",
      "Stopping. Best iteration:\n",
      "[101]\tvalidation_0-rmse:0.246986\tvalidation_1-rmse:0.303039\n",
      "\n",
      "[0]\tvalidation_0-rmse:0.844233\tvalidation_1-rmse:0.871277\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 1000 rounds.\n",
      "Stopping. Best iteration:\n",
      "[23]\tvalidation_0-rmse:0.254226\tvalidation_1-rmse:0.294708\n",
      "\n",
      "[0]\tvalidation_0-rmse:0.847506\tvalidation_1-rmse:0.838659\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 1000 rounds.\n",
      "Stopping. Best iteration:\n",
      "[18]\tvalidation_0-rmse:0.259822\tvalidation_1-rmse:0.271632\n",
      "\n",
      "[0]\tvalidation_0-rmse:0.843249\tvalidation_1-rmse:0.871691\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 1000 rounds.\n",
      "Stopping. Best iteration:\n",
      "[21]\tvalidation_0-rmse:0.254543\tvalidation_1-rmse:0.275271\n",
      "\n",
      "[0]\tvalidation_0-rmse:0.843331\tvalidation_1-rmse:0.876932\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 1000 rounds.\n",
      "Stopping. Best iteration:\n",
      "[20]\tvalidation_0-rmse:0.258099\tvalidation_1-rmse:0.278123\n",
      "\n",
      "[0]\tvalidation_0-rmse:0.851405\tvalidation_1-rmse:0.792233\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 1000 rounds.\n",
      "Stopping. Best iteration:\n",
      "[14]\tvalidation_0-rmse:0.257479\tvalidation_1-rmse:0.291365\n",
      "\n",
      "[0]\tvalidation_0-rmse:0.848008\tvalidation_1-rmse:0.833928\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 1000 rounds.\n",
      "Stopping. Best iteration:\n",
      "[152]\tvalidation_0-rmse:0.240847\tvalidation_1-rmse:0.273922\n",
      "\n",
      "[0]\tvalidation_0-rmse:0.845623\tvalidation_1-rmse:0.865179\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 1000 rounds.\n",
      "Stopping. Best iteration:\n",
      "[18]\tvalidation_0-rmse:0.255237\tvalidation_1-rmse:0.274256\n",
      "\n",
      "[0]\tvalidation_0-rmse:0.848766\tvalidation_1-rmse:0.823467\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 1000 rounds.\n",
      "Stopping. Best iteration:\n",
      "[14]\tvalidation_0-rmse:0.260875\tvalidation_1-rmse:0.307788\n",
      "\n",
      "[0]\tvalidation_0-rmse:0.847506\tvalidation_1-rmse:0.845622\n",
      "Multiple eval metrics have been passed: 'validation_1-rmse' will be used for early stopping.\n",
      "\n",
      "Will train until validation_1-rmse hasn't improved in 1000 rounds.\n",
      "Stopping. Best iteration:\n",
      "[31]\tvalidation_0-rmse:0.249899\tvalidation_1-rmse:0.314957\n",
      "\n",
      "CV score: 0.08346703885153668\n"
     ]
    }
   ],
   "source": [
    "MSE = 0\n",
    "stack_preds = np.zeros(len(test_data))\n",
    "feature_importance_stack = pd.DataFrame()\n",
    "\n",
    "n_fold = 10\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=0)\n",
    "for fold, (train_idx, valid_idx) in enumerate(folds.split(train_stack)):\n",
    "    X_train, y_train = train_stack.iloc[train_idx], y_t.iloc[train_idx]\n",
    "    X_valid, y_valid = train_stack.iloc[valid_idx], y_t.iloc[valid_idx]\n",
    "    model = xgb.XGBRegressor(**xgb_params2, n_estimators=20000, n_jobs=3)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "              verbose=10000, early_stopping_rounds=1000)\n",
    "    val_pred = model.predict(X_valid)\n",
    "    fold_importance = pd.DataFrame()\n",
    "    fold_importance['feature'] = train_stack.columns\n",
    "    fold_importance['importance'] = model.feature_importances_\n",
    "    fold_importance['fold'] = fold + 1\n",
    "    MSE += mean_squared_error(y_valid, val_pred) / n_fold\n",
    "    stack_preds += model.predict(test_stack) / n_fold\n",
    "    feature_importance_stack = pd.concat([feature_importance_stack, fold_importance], axis=0)\n",
    "    \n",
    "print('CV score: {}'.format(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(stack_preds).to_csv('stack_lgb_xgb_rf_kr_gbm_svr12.txt', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(16, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance_stack.sort_values(by='importance', ascending=False))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running 3000 rounds of LightGBM parameter optimisation:\n",
      "100%|█| 3000/3000 [31:12<00:00,  1.02s/it, best loss: 0.3150225250720053] \n",
      "{bagging_fraction: 0.58\n",
      "boosting: gbdt\n",
      "feature_fraction: 0.59\n",
      "lambda_l1: 0.0027492236181156596\n",
      "lambda_l2: 0.09922282062823436\n",
      "learning_rate: 0.10149545781981265\n",
      "max_bin: 247\n",
      "max_depth: 18\n",
      "metric: RMSE\n",
      "min_data_in_bin: 20\n",
      "min_data_in_leaf: 6\n",
      "min_gain_to_split: 0.1\n",
      "num_leaves: 73\n",
      "objective: huber\n",
      "subsample: 0.6968388320474507}\n"
     ]
    }
   ],
   "source": [
    "lgb_params2 = quick_hyperopt(train_stack, y_t, package='lgbm', num_evals=3000, diagnostic=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bagging_fraction': 0.58,\n",
       " 'boosting': 'gbdt',\n",
       " 'feature_fraction': 0.59,\n",
       " 'lambda_l1': 0.0027492236181156596,\n",
       " 'lambda_l2': 0.09922282062823436,\n",
       " 'learning_rate': 0.10149545781981265,\n",
       " 'max_bin': 247,\n",
       " 'max_depth': 18,\n",
       " 'metric': 'RMSE',\n",
       " 'min_data_in_bin': 20,\n",
       " 'min_data_in_leaf': 6,\n",
       " 'min_gain_to_split': 0.1,\n",
       " 'num_leaves': 73,\n",
       " 'objective': 'huber',\n",
       " 'subsample': 0.6968388320474507}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_params2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[44]\ttraining's rmse: 0.221508\tvalid_1's rmse: 0.310171\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[57]\ttraining's rmse: 0.2204\tvalid_1's rmse: 0.296692\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[54]\ttraining's rmse: 0.222242\tvalid_1's rmse: 0.272112\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[56]\ttraining's rmse: 0.220091\tvalid_1's rmse: 0.287977\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[48]\ttraining's rmse: 0.221886\tvalid_1's rmse: 0.282137\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[55]\ttraining's rmse: 0.221928\tvalid_1's rmse: 0.29412\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[56]\ttraining's rmse: 0.221812\tvalid_1's rmse: 0.276331\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[52]\ttraining's rmse: 0.221688\tvalid_1's rmse: 0.270975\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[51]\ttraining's rmse: 0.219884\tvalid_1's rmse: 0.308032\n",
      "Training until validation scores don't improve for 1000 rounds.\n",
      "Early stopping, best iteration is:\n",
      "[57]\ttraining's rmse: 0.220285\tvalid_1's rmse: 0.321469\n",
      "CV score: 0.08553281720926939\n"
     ]
    }
   ],
   "source": [
    "MSE = 0\n",
    "stack_preds = np.zeros(len(test_data))\n",
    "feature_importance_stack = pd.DataFrame()\n",
    "\n",
    "n_fold = 10\n",
    "folds = KFold(n_splits=n_fold, shuffle=True, random_state=0)\n",
    "for fold, (train_idx, valid_idx) in enumerate(folds.split(train_stack)):\n",
    "    X_train, y_train = train_stack.iloc[train_idx], y_t.iloc[train_idx]\n",
    "    X_valid, y_valid = train_stack.iloc[valid_idx], y_t.iloc[valid_idx]\n",
    "    model = lgb.LGBMRegressor(**lgb_params2, n_estimators=20000, n_jobs=3)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "              verbose=10000, early_stopping_rounds=1000)\n",
    "    val_pred = model.predict(X_valid)\n",
    "    fold_importance = pd.DataFrame()\n",
    "    fold_importance['feature'] = train_stack.columns\n",
    "    fold_importance['importance'] = model.feature_importances_\n",
    "    fold_importance['fold'] = fold + 1\n",
    "    MSE += mean_squared_error(y_valid, val_pred) / n_fold\n",
    "    stack_preds += model.predict(test_stack) / n_fold\n",
    "    feature_importance_stack = pd.concat([feature_importance_stack, fold_importance], axis=0)\n",
    "    \n",
    "print('CV score: {}'.format(MSE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(stack_preds).to_csv('stack_lgb_xgb_rf_kr_gbm_svr12_2.txt', index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
